% tutorial.tex  -  a short descriptive example of a LaTeX document
%
% For additional information see  Tim Love's ``Text Processing using LaTeX''
% http://www-h.eng.cam.ac.uk/help/tpl/textprocessing/
%
% You may also post questions to the newsgroup <b> comp.text.tex </b> 

\documentclass[12pt]{article}			% For LaTeX 2e
						% other documentclass options:
						% draft, fleqn, openbib, 12pt

\usepackage{graphicx}	 			% insert PostScript figures
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subfig}
\usepackage{setspace}
\usepackage{fancybox}
\usepackage{hyperref}
%% \usepackage{setspace}   % controllabel line spacing
%% If an increased spacing different from one-and-a-half or double spacing is
%% required then the spacing environment can be used.  The spacing environment 
%% takes one argument which is the baselinestretch to use,
%%         e.g., \begin{spacing}{2.5}  ...  \end{spacing}


% the following produces 1 inch margins all around with no header or footer
\topmargin	=0.mm		% beyond 25.mm
\oddsidemargin	=0.mm		% beyond 25.mm
\evensidemargin	=0.mm		% beyond 25.mm
\headheight	=0.mm
\headsep	=0.mm
\textheight	=220.mm
\textwidth	=165.mm
					% SOME USEFUL OPTIONS:
% \pagestyle{empty}			% no page numbers
 \parindent  15.mm			% indent paragraph by this much
 \parskip     2.mm			% space between paragraphs
% \mathindent 20.mm			% indent math equations by this much

\newcommand{\MyTabs}{ \hspace*{25.mm} \= \hspace*{25.mm} \= \hspace*{25.mm} \= \hspace*{25.mm} \= \hspace*{25.mm} \= \hspace*{25.mm} \kill }

\graphicspath{{../Figures/}{../data/:}}  % post-script figures here or in /.

					% Helps LaTeX put figures where YOU want
 \renewcommand{\topfraction}{0.9}	% 90% of page top can be a float
 \renewcommand{\bottomfraction}{0.9}	% 90% of page bottom can be a float
 \renewcommand{\textfraction}{0.1}	% only 10% of page must to be text
\renewcommand{\thesubfigure}{\thefigure.\arabic{subfigure}}
\captionsetup[subfigure]{labelformat=simple,labelsep=colon,
listofformat=subsimple}
\captionsetup{lofdepth=2}
\makeatletter
\renewcommand{\p@subfigure}{}
\makeatother

\linespread{1.5}
\alph{footnote}				% make title footnotes alpha-numeric

\title{Continuous Multimodal User Authentication}	% the document title
\author{{\bf Project report}{\bf}}
\date{}				% your own text, a date, or \today

% --------------------- end of the preamble ---------------------------

\begin{document}			% REQUIRED

\begin{singlespacing}
\pagenumbering{roman}			% Roman numerals from abstract to text
% \maketitle				% you need to define \title{..}
\thispagestyle{empty}			% no page number on THIS page 

\begin{center}

% \begin{doublespacing}
%A synopsis submitted in partial fulfillment of the requirements of the course \\[3ex]
% \thisfancyput(3.25in,-4.5in){%
%   \setlength{\unitlength}{1in}\fancyoval(7.5,9.5)}%
\thisfancypage{%
\setlength{\fboxsep}{9pt}%
\setlength{\shadowsize}{8pt}%
\doublebox}{}
\textbf{\Large Continuous Multimodal User Authentication}\\[4ex]
\textit{\large A project report submitted to}\\[2ex]
\textbf{\Large M S Ramaiah Institute of Technology}\\[2ex]
An Autonomous Institute, Affiliated to\\[2ex]
\textbf{Visvesvaraya Technological University, Belgaum}\\[2ex]
\textit{in partial fulfillment for the award of the degree of}\\[2ex]
\textbf{\textit{\large Bachelor of Engineering in Computer Science \& Engineering}}\\[4ex]
Submitted by\\[2ex]
\begin{table}[htp]
	\centering
	\begin{tabular}{lcc}
	Soumya G       &  \hspace{20 mm} & 1MS08CS119             \\
	Tribhuvanesh O &  \hspace{20 mm} & 1MS08CS129             \\ 
	\end{tabular}
\end{table}

{\bf Under the guidance of }{\bf}\\[2ex]
Dr. K. G. Srinivasa\\
Professor\\
Department of Computer Science and Engineering\\
M. S. Ramaiah Institute of Technology\\[3ex]


\includegraphics[scale=0.06]{img/msrit.jpg}\\
\uppercase{\textbf{Department of Computer Science \& Engineering}}\\[2ex]
\uppercase{\textbf{M. S. Ramaiah Institute of Technology}}\\
\textbf{(Autonomous Institute Affiliated to VTU)}\\
\textbf{Bangalore - 560054}\\
\url{www.msrit.edu}\\
{May 2012}
\end{center}
\end{singlespacing}
\newpage

%
% Certificate 
%
\begin{center}
\uppercase{\textbf{Department of Computer Science \& Engineering}}\\[2ex]
\uppercase{\textbf{M. S. Ramaiah Institute of Technology}}\\
\textbf{(Autonomous Institute Affiliated to VTU)}\\
\textbf{Bangalore - 560054}\\[30ex]
\textbf{CERTIFICATE}\\[4ex]
\end{center}
This is to certify that the project work titled
\textbf{Continuous Multimodal User Authentication}
is carried out by
\textbf{Soumya G} (1MS08CS119) and
\textbf{Tribhuvanesh O} (1MS08CS129)
in partial fulfillment for the award of degree of Bachelor of
Engineering in Computer Science and Engineering during the year 2012. The Project
report has been approved as it satisfies the academic requirements with respect to
the project work prescribed for Bachelor of Engineering Degree. To the best of our
understanding the work submitted in this report has not been submitted, in part or full, for
the award of any diploma or degree of this or any other University.\\[5ex]
\begin{center}
\begin{table}[htp]
	\centering
	\begin{tabular}{lcc}
	(Dr. K.G.Srinivasa)       &  \hspace{90 mm} & (Dr. R.Selvarani)             \\
	Guide                     &  \hspace{90 mm} & Head, Dept. of CSE             \\ 
	\end{tabular}
\end{table}

\vspace{6 mm}
(External Examiner)
\end{center}

\newpage
\begin{center}
\uppercase{\textbf{\large Declaration}}
\end{center}
We hereby declare that the entire work embodied in this report has been carried out by
us at M. S. Ramaiah Institute of Technology under the supervision of
Dr.K.G.Srinivasa.
This report has not been submitted in part or full for the award of any
diploma or degree of this or any other University.

\vspace{5 mm}
\noindent{}1MS08CS119 - Soumya G\\
1MS08CS129 - Tribhuvanesh O

\newpage
%\addcontentsline{toc}{chapter}{\numberline{}Abstract}
% TODO <-----ABSTRACT GOES HERE------>

\begin{abstract}
\section*{}
\addcontentsline{toc}{subsection}{Abstract}
Static Authentication methods, although providing a rigid and secure framework to this one-time authentication session, do not provide authentication of the user throughout the session. This leaves the possibility of an imposter gaining access in multiple scenarios.\\
The goal of \emph{Continuous Authentication} is to authenticate the user right from the initial stages of log-in through log-out.
This can be implemented by extrapolating the tried-and-tested static authentication techniques thoughout the session.
But, this introduces new challenges, since these "one-time authentication" techniques are computationally expensive, restricts the user's movement and postures in front of the system, require extra expensive hardware and deviates the user from his work-flow.
In these situations, the user no longer remains uninterrupted by the authentication process in the background.\\
This project proposes Continuous Authentication, using login through conventional passwords followed by authentication using two modes - hard and soft biometrics till logout. The hard biometric trait - facial features, is chosen so that the user need not invest in any additional expensive hardware. The noise inherent in the process of face recognition, is managed by using a machine learning algorithm which captures the temporal data and expresses it as confidence. The soft traits are used in phases when this confidence is high to relieve the CPU of comparatively high computation.

\end{abstract}				% end of the abstract
\newpage
\section*{}
\addcontentsline{toc}{subsection}{Acknowledgements}
\begin{center}
{\LARGE \bf Acknowledgements}
\\[6ex]
\end{center}
We would like to thank our project guide, Dr. K.G.Srinivasa, for his valuable and much-needed guidance and patience during the course of this project. We are grateful for the timely inputs we received from him, right from the start of the project.\\[2ex]
We are thankful to the Department of Computer Science and Engineering, and our Head of Department, Dr. R. Selvarani, for having provided us an opportunity to work on this project and enhance our skills in this field.\\[2ex]
We also thank the lab assistants and our friends for their immense help and suggestions that helped us further improve this project. 

\newpage				% start a new page

\section*{}
\addcontentsline{toc}{subsection}{Contents}
\tableofcontents			% create table of contents automatically

\newpage				% start a new page
%\listoffigures				% to create list of figures automatically
\section*{}
\addcontentsline{toc}{subsection}{List of Figures}
\listoffigures

%list of tables				% create list of tables

\newpage				% start a new page
\section*{}
\addcontentsline{toc}{subsection}{List of Tables}
\listoftables
\newpage				% start a new page
\pagenumbering{arabic}			% Arabic page numbers from now on

\section{ Introduction }	% start the first numbered section

% <-------- INTRODUCTION -------->

\subsection{ General Introduction }

Authentication in the context of computer security is a process which verifies the claimed identity of the user. Upon successfule authentication, the user is granted privilieges enabled by a higher authority. A number of elements together can decide the authenticity of the user before authorizing him/her. These elements can be classified based on three factors called \emph{authentication factors}.
Security research has determined that for a positive identification, atleast two of the three authentication factors need to be satisfied.
These factors are:

\begin{itemize}
	\item \textbf{Knowledge factors}: Something the user \emph{knows} (e.g., Username-password pair)
	\item \textbf{Ownership factors}: Something the user \emph{has} (e.g., ID card, cell phone, security token)
	\item \textbf{Inherence factors}: Something the user \emph{is} (e.g., Fingerprint, retinal patterns, facial features)
\end{itemize}

The purpose of this work is to extend the conventional user authentication techniques using these factors in an unobtrusive way.
This brings out the need to delineate the concepts of \emph{static} and \emph{continuous} authentication.
%Klosterman and Granger in their study\cite{Klos00} dilineate authentication based on static and continuous methods.
\emph{Static Authentication} refers to the method of authenticating a user, at the time of log-in. In most cases, knowledge-based methods such as passwords are used since every user can verify his claim in a very convenient manner. But, passwords and knowledge factors lose their credibility when shared, forgotten or stolen. Similarly, ownernership-based methods too can duplicated, stolen or lost.
When dealing with sensitive content one might resort to additional equipment to verify the user based on their unique traits such as fingerprints or retinal patterns. However, in this case, the only period during which system is very confident and fully aware of user's identity claim is during the authentication period.
For example, computer setups nowadays use a 2-factor authentication technique by extracting the password and the user's facial features at the time of login and assumes it is the same authenticated user till logout. But when the user moves away from the system to take a break without logging-out, it is susceptible to tailgating when an imposter takes the authenticated user's place. This could prove to be a critical security weakness in  high-security systems.

In order to address this issue, the system needs to continually verify the user's identity claim in an unobtrusive manner.

Studies about the concept and issues of implementing \emph{Continuous Authentication} by Klosterman et al.\cite{Klos00}, showed an elegant approach of using biometrics to verify the user's identity through-out the session. Not all modes of "biometrics" can achieve this goal since methods like finger-prints and retinal scans distracts the user and he/she no longer remains uniterrupted by the authentication process in the background.
A lot of research and experimentation has been carried out in the field of Continuous Authentication \cite{Niin10,Klos00,mon00,turk03,sim07,azz08,azz082}, though it has not been adopted for widespread usage as readily as the Static Authentication systems have been, mainly due to inconvenience.

Before proceeding to explain the challenges posed by the development of a Continuous Authentication system, an overview of \emph{Biometric traits} in the context of Continuous Authentication is necessary.
\emph{Biometric traits} refer to the physiological or behavioural traits of a user that can identify a user, for a session.
These traits can be divided into the following two categories:
\begin{itemize}
	\item {\bf Hard Biometric traits}: Physical traits of a user that are assumed to be present universally and can uniquely identify an individual. For example, fingerprints, facial features, DNA and so on.
	\item {\bf Soft Biometric traits}: These are characteristics of a user that "provide some information about the individual, but lack the distinctiveness and permanence to sufficiently differentiate any two individuals"\cite{Jain204}. For example, colour of clothing/skin/eye/hair, gender and other such factors.
\end{itemize}

The study conducted by Klosterman et al.\cite{Klos00} in the design of biometric-enhanced authentication system also describes the challenges posed by such a system.
They point out that for unobtrusive continuous monitoring of a user's biometric traits, the trait cannot be something that needs to be in contact with a sensor.
Therefore, facial features of a user make for an ideal choice for continuous unobtrusive monitoring.
Another important observation in the previously cited paper is the fact that biometrics are expensive to compute.
In case of facial features, the image processing and recognition algorithms can be computationally much more expensive as compared to password verification.
Therefore, we intend to incorporate Soft Biometric Authentication in our system, which is computationally inexpensive and also allows the user more flexibilty in posture. 

The feasibility of using soft biometric traits for user identification was researched by \emph{Jain et al.}\cite{Jain204}, and it was found to reduce computational costs of recognition.
A more detailed implementation of authentication incorporating soft biometric traits has been researched by \emph{Niinuma et al.}\cite{Niin10}.
They create a template of the user's shirt and skin colour and generate "similarity scores" for the subsequent frames in which the user is captured.
The user is identified based on the comparison of the similarity scores with a certain threshold value.
While this model yields fairly good results, according to the results, it leaves scope for improvement of recognition by considering temporal information.
By temporal information, we mean a technique similar to a sliding window which analyzes patterns around the time the authentication state is to be predicted.
Hence, this implies a decay of data beyond a certain time in the past.

We address this problem by developing a Continuous Authentication system, wherein, a user at the time of login initiates a session by entering the right username-password combination.
Upon successful login, provided the user's identity is valid, the system proceeds into a hard biometrics mode and subsequently a soft biometrics mode.
This work incorporates the use of facial features and the colour of the user's clothing in these two modes repectively. 
Furthermore, a supervised machine learning algorithm has been used to bridge the gap between the two modes.
This enables the system to use any face recognition algorithm which produces the desired output as explained in the later sections.
The temporal information suggested earlier is used by this machine learning algorithm to predict the authentication state at any given point in time.
When the confidence of the system in the hard biometrics mode is beyond the specified threshold, the system enrolls the soft biometric template of the user and transitions into the soft biometrics mode.

\subsection{ Statement of the Problem }

Soft biometric traits don't provide as high a level of confidence as hard biometric traits. \cite{Niin10} suggests using these soft-biometric traits throughout the authenticated period to provide high tolerance to the user's posture in front of the computer system. Albeit, this being true as shown in our experiments, the problem is to incorporate both traits during the authenticated period, switching between them as and when necessary, depending on the confidence calculated using an appropriate mathematical model.

Hard biometric traits, which in this case is face recognition using Eigenfaces\cite{Turk91} is used over its counterparts, such as iris and fingerprint based recognition, since these options are not only expensive, but also cannot be used during the authenticated session without interrupting the user from his work-flow. 
Thw disadvantage of solely running on face recognition lies in the fact that its computationally expensive and posture-dependent. The problem hence extends to take into account these factors when building the system.

\subsection{ Objectives of the Project }
With the statement of the problem as described earlier, this objectives of this project can be tersely stated as follows:
\begin{itemize}
\item A low-cost solution with the user not compelled to invest in expensive or extra hardware.
\item Incorporating techniques to intelligently authenticate the user throughout the session along with the conventional and time-tested method of a one-time password system, which is to be the first step during log-in.
\item The one-time password based authentication step to be followed by analyzing the unique-per-user hard biometric traits to achieve a high level of confidence and proceed to soft-biometric measures to maintain that confidence.
\item Instead of bifurcating the hard and soft biometric authentication phases into clearly distinct phases as suggested by \cite{Niin10}, using a suitable model to shift between the two phases and maintain that level of confidence without the user's work-flow being interrupted unless there exists a large uncertainity.
\item To allow the theory to be extended to include more modalities.
\end{itemize}

\subsection{ Current scope }
As a part of the proposed Continuous Authentication, the following falls in its scope:
\begin{itemize}
	\item An administration module to add and remove users.
	\item Collect and learn the facial features of the user at the time of account-creation
	\item SHA1 encrypted passwords stored in the database, an XML file, which can't be reverse-engineered.
	\item Deny permission to the user unless authentication with strong confidence.
	\item Use temporal information to dampen noise. A machine learning algorithm, which in our case is a Support Vector Machine, uses the output produced by face recognition over a time period $T$ with each atomic time slice providing information on recognized person and confidence.
\end{itemize}

\noindent The following features do not fall in the scope of our project:
\begin{itemize}
	\item A full-fledged front-end integrated with advanced back-end measures such as PolicyKit or dbus. 
	\item Certain scenarios that haven't been yet captured in the training data. (But these can be easily trained and included in the next run)
	\item Ability to deal with contrasting changes in user's facial features or surroundings.
	\item A relational database to store facial features
\end{itemize}

\subsection{ Future scope } 
This project allows scope for improving both the Hard and Soft Biometric Authentication modules by
\begin{itemize}
\item Changing the implementation of the Face Recognition module by replacing it with a different algorithm with better accuracy.
\item Adding more modules which incorporate more biometric features of the user to predict the authenticity of the user with greater accuracy. The result of this can be used by the SVMs to dampen noise and maintain the Continuous Authentication system's confidence level in the user.  
\end{itemize}

\newpage

\section{Literature Survey }
\subsection{ Authentication }
Authentication is the process of verifying the claimed identity of a user. The network elements (NE) environment must offer features to verify the claimed identity of a user before giving that user operations access. Depending on the NE and the applications, there could be different kinds of authenticators. For example, according to \cite{war02}
\begin{itemize}
\item The user can be associated with confidential information that only he or she is supposed to posses such as: password, private key, or randomly time-varying PIN (such as those provided by single-use password tokens) 
\item The user can be associated with a distinctive physical or logical address (e.g., user’s authorized directory number, network address)
\item The user can be authenticated by certain unique attributes such as: voice or speech pattern, handwriting style, palm print, or retina scan.
\end{itemize}
There are three fundamental techniques used in authentication mechanisms\cite{john03}:
\begin{itemize}
\item Something you know, which usually refers to passwords and PINs. The simplest implementations of passwords and personal identification numbers (PINs) yield the
simplest of all authentication mechanisms.
\item Something you have, which usually refers to cards or tokens. Physical authentication devices, such as smart cards and password tokens, were developed to eliminate certain weakness associated with passwords. A major benefit of cards and tokens is that they can’t be shared with the same freedom as sharing passwords.
\item Something you are, which refers to biometrics - the measurement of physical characteristics or personal traits. Common biometric verification techniques try to match measurements from one user’s fingerprint, hand, eyes, face, or voice to measurements that were previously collected from him/her.
\end{itemize}
There are two general applications for this: identification and verification. “With identification, the biometric system asks and attempts to answer the question, ‘who is X?’ Verification occurs when the biometric system asks and attempts to answer the question, ‘is this X?’ after the user claims to be X. In a verification application, the biometric system requires input from the user, at which time the user claims his or her identity via a password, token, or user name. This user input points the system to a template in the database. The system also requires a biometric sample from the user. It then processes and compares the sample to or against the user. This is called a “one-to-one search (1:1)”

\subsection{ Authentication methods }
All methods of authentication require you to specify who or what you are and to relay appropriate credentials to prove that you are who you say you are. These credentials generally take the form of something you know, something you have, or something you are. What you know may be a password. What you have could be a smart card. What you are pertains to the field of biometrics. The important element is to recognize that different mechanisms provide authentication services with varying degrees of certainty. Choosing the proper authentication technology largely depends on the location of the entities being authenticated and the degree of
trust placed in the particular facets of the network.\\
The following sections delve into key methods used in this project.

\subsubsection{ Username and Password authentication }
Username and password are often used in the practical world. It’s a method which is based on “what you know” for authentication. It is the simplest way of authentication and it provides each user with a unique username and a secret password. It is an easy way to get attacked by password guessing. As long as a user can enter the correct username and password, the computer and system will assume that the operator who using the computer is the legal user or original user.

\subsubsection{ Biometrics }
Biometrics is first introduced in the 1970s and early 1980s. This technology gathers unique physiological or behavioral attributes of a person for storing in a database or comparing it with data already found in the database. Same as test procedure reference, a biometric is \emph{“defined as a unique, measurable, biological characteristic or trait for automatically recognizing or verifying the identity of a human being. Statistically analyzing these biological characteristics has become known as the science of biometrics.”} These days, biometric technologies are typically used to analyze human characteristics for security purposes. Five of the most common physical biometric patterns analyzed for security purposes are fingerprint, hand, eye, face, and voice.\\
Biometrics can, in our case, be classified into \emph{Hard Biometric traits}, which remain unique-per-user and \emph{Soft Biometric traits}, which remain unique-per-session.

\subsubsection{ Hard Biometrics}
Hard biometric traits refer to those features of the user that remains unique among all sessions and are assumed to be present universally among users. These can be fingerprints, iris pattern, DNA, or even facial features. Features like fingerprints or iris pattern are considered ideal choices for static authentication using biometrics. Many studies \cite{war02,john03,way97,kang06} show good results with using biometric traits for authentication. Traits like facial features which can be captured unobtrusively, make for a good choice for Continuous Authentication Systems. Researcher Mario Savvides, an Associate Research Professor in the Carnegie Mellon university has been a part of many projects and published various papers in the field of Face Recognition, Continuous Authentication and Iris Recognition \cite{marsav}. 

\subsubsection{ Soft Biometrics }
The first personal identification system developed by Alphonse Bertillon\cite{bert96} for identification of criminals was based on three sets of features: (i) body measurements (anthropometry) like height and length of the arm, (ii) morphological description of the appearance and shape of the body like eye color and anomalies of the fingers, and (iii) peculiar marks observed on the body like moles, scars, and tattoos. Although the Bertillon system was very useful in tracking criminals, it had an unacceptably high rate of false identification. This was due to two reasons. Firstly, several individuals can have the same set of values for these measurements. Secondly, for the same individual, these values can change over time. In other words, these characteristics do not have the distinctiveness and permanence to uniquely identify an individual over a period of time and hence we refer them as soft biometric traits. {\bf \it Soft biometric traits are those
characteristics that provide some information about the individual, but lack the distinctiveness and permanence to sufficiently differentiate any two individuals.}\cite{Jain204}\\
The soft biometric traits can either be continuous or discrete. Traits such as gender, eye color, ethnicity, etc. are discrete in nature. On the other hand, traits like height and weight are continuous variables. A system that is completely based on soft biometric traits cannot provide the required accuracy in the recognition of individuals.\\
Wayman\cite{way97} proposed the use of soft biometric traits like gender and age, for filtering a large biometric database. Filtering refers to limiting the number of entries in a database to be searched, based on characteristics of the interacting user. For example, if the user can somehow be identified as a middle-aged male, the search can be restricted only to the subjects with this profile enrolled in the database. This greatly improves the speed or the search efficiency of the biometric system. Filtering reduces the probability of obtaining a wrong match, but this is offset by the fact that the errors in filtering also reduce the probability of obtaining a correct match. Hence, in general, filtering drastically reduces the time required for identification but can degrade the recognition performance.

\subsection{ Continuous Authentication Systems }
\begin{figure}
	\centering
		\includegraphics[scale=0.6]{img/ca1.png}
	\caption{Continuous Authentication}
\end{figure}
Continuous Authentication system have gained momentum over the recent years due to the advancement in technology to handle computationally expensive tasks, as well as the need to secure the vast amount of private information that can be exchanged during an authenticated session.\\
Various studies\cite{mon00,turk03,sim07,azz08,azz082,kang06,car03} have been carried out on the problem of Continuous Authentication.
Sensible Vision's \emph{Fast Access} provides an industrial solution, with the solution involving a proprietay set-up and being a paid solution.

\subsection{ Face Detection }
OpenCV's face detector uses a method that Paul Viola and Michael Jones published in 2001\cite{Viola01}. Usually simply called the Viola-Jones method, or even just Viola-Jones, this approach to detecting objects in images combines four key concepts that can be summarized the four key-steps.
\begin{itemize}
	\item Simple rectangular features, called Haar features
	\item An Integral Image for rapid feature detection
	\item The AdaBoost machine-learning method
	\item A cascaded classifier to combine many features efficiently
\end{itemize}
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{img/fd5.png}
	\caption{Examples of Haar features in OpenCV}
	\label{fig:fd1}
\end{figure}
The features that Viola and Jones used are based on Haar wavelets. Haar wavelets are single wavelength square waves (one high interval and one low interval). In two dimensions, a square wave is a pair of adjacent rectangles - one light and one dark.\\
The actual rectangle combinations used for visual object detection are not true Haar wavlets. Instead, they contain rectangle combinations better suited to visual recognition tasks. Because of that difference, these features are called Haar features, or Haarlike features, rather than Haar wavelets. Figure \ref{fig:fd1} shows the features that OpenCV uses.The presence of a Haar feature is determined by subtracting the average dark-region pixel value from the average light-region pixel value. If the difference is above a threshold (set during learning), that feature is said to be present.\\
To determine the presence or absence of hundreds of Haar features at every image location and at several scales efficiently, Viola and Jones used a technique called an Integral Image. In general, "integrating" means adding small units together. In this case, the small units are pixel values. The integral value for each pixel is the sum of all the pixels above it and to its left. Starting at the top left and traversing to the right and down, the entire image can be integrated with a few integer operations per pixel.\\
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.3]{img/fd3.png}
	\caption{The Integral Image trick}
	\label{fig:fd2}
\end{figure}
As Figure \ref{fig:fd2} shows, after integration, the value at each pixel location, (x,y), contains the sum of all pixel values within a rectangular region that has one corner at the top left of the image and the other at location (x,y). To find the average pixel value in this rectangle, you'd only need to divide the value at (x,y) by the rectangle's area.\\
But what if you want to know the summed values for some other rectangle, one that doesn't have one corner at the upper left of the image? Figure 2b shows the solution to that problem. Suppose you want the summed values in $D$. You can think of that as being the sum of pixel values in the combined rectangle, $A+B+C+D$, minus the sums in rectangles $A+B$ and $A+C$, plus the sum of pixel values in $A$. In other words,
\begin{center}
  $D = A+B+C+D -  (A+B) -  (A+C) + A$
\end{center}
Conveniently, $A+B+C+D$ is the Integral Image's value at location 4, $A+B$ is the value at location 2, $A+C$ is the value at location 3, and $A$ is the value at location 1. So, with an Integral Image, you can find the sum of pixel values for any rectangle in the original image with just three integer operations:
\begin{center}
\(
\left(x4, y4\right) - \left(x2, y2\right) - \left(x3, y3\right) + \left(x1, y1\right)
\).
\end{center}
To select the specific Haar features to use, and to set threshold levels, Viola and Jones use a machine-learning method called AdaBoost. AdaBoost combines many "weak" classifiers to create one "strong" classifier. "Weak" here means the classifier only gets the right answer a little more often than random guessing would. That's not very good. But if you had a whole lot of these weak classifiers, and each one "pushed" the final answer a little bit in the right direction, you'd have a strong, combined force for arriving at the correct solution. AdaBoost selects a set of weak classifiers to combine and assigns a weight to each. This weighted combination is the strong classifier.\\
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{img/fd4.png}
	\caption{\small The classifier cascade is a chain of filters. Image subregions that make it through the entire cascade are classified as "Face." All others are classified as "Not Face."}
	\label{fig:fd3}
\end{figure}
Viola and Jones combined a series of AdaBoost classifiers as a filter chain, shown in Figure \ref{fig:fd3}, that's especially efficient for classifying image regions. Each filter is a separate AdaBoost classifier with a fairly small number of weak classifiers.\\
The acceptance threshold at each level is set low enough to pass all, or nearly all, face examples in the training set. The filters at each level are trained to classifiy training images that passed all previous stages. (The training set is a large database of faces, maybe a thousand or so.) During use, if any one of these filters fails to pass an image region, that region is immediately classified as "Not Face." When a filter passes an image region, it goes to the next filter in the chain. Image regions that pass through all filters in the chain are classified as "Face." Viola and Jones dubbed this filtering chain a cascade.\\
The order of filters in the cascade is based on the importance weighting that AdaBoost assigns. The more heavily weighted filters come first, to eliminate non-face image regions as quickly as possible. Figure 4 shows the first two features from the original Viola-Jones cascade superimposed on my face. The first one keys off the cheek area being lighter than the eye region. The second uses the fact that the bridge of the nose is lighter than the eyes.


\subsection{ Face Recognition using Eigenfaces }
Eigenfaces is a face recognition algorithm that was first described by Turk and Pentland in their paper published in 1991. It works to capture the variations present among the images of faces, that form the training set. It uses this information to create a face model where each image is represented as an eigenvector in what is called a "PCA subspace". Recognition of a test face is carried out by converting the test image into a similar eigenvector and then comparing its distance from all others in the subspace. The person corresponding to closest image is said to be the person recognized in the test image. It encodes the complete face characteristics, as opposed to capturing features of the face separately.\\

\subsubsection{ Algorithm for Eigenfaces }
The steps for recognizing an face using Eigenfaces are described below in an abstract manner:
\begin{enumerate}
\item Acquire the training set of face images and calculate the “eigenfaces”, which define the “face space”. 
\item When a new image is encountered, calculate the set of weights based on the input image and all the “eigenfaces” by projecting the input image onto each of the “eigenfaces”.
\item Determine if the image is a face at all (whether known or unknown) by checking if the image is sufficiently close to the “face space”. 
\item If it is a face, classify the weight pattern as either a known or unknown person. 
\end{enumerate}

\subsubsection{ Computing Eigenfaces }
Eigenfaces can be computed in the following manner:
\begin{enumerate}
	\item A training set of images {\bf S} containing $\tau_{1},\tau_{2},..\tau_{M}$ total faces of the users to be recognized is prepared. These faces are captured during account creation of a user and converted to grayscale and equalized and resized to a standard resolution. Each image is converted to a vector of size {\bf N} by concatenating all the pixels row by row. These vectors are put into a matrix {\bf T} with each row representing an image.
	\item The average of all vectors $\psi$ is calculated and subtracted from each of the vectors in {\bf T} to obtain $\phi_{i}, i = 1,2,..,n$.
	\item The eigenvectors $u_{k}$ and eigenvalues $\lambda_{k}, k = 1,..,M$ of the co-variance matrix {\bf C} are calculated. The covariance matrix itself is found by: 
\begin{equation}
{\bf C} = \frac{1}{M}\sum_{n=1}^{M}\phi_{n}\phi_{n}^{T}
\end{equation}
	\item Since the dimension of C is very high (of the order of the number of pixels in the image), another matrix {\bf L} with the dimensions $MxM$ is constructed on basis of analysis given in \cite{Turk91}, 
	\begin{equation}
	{\bf L} = {\bf A}^{T}{\bf A}
	\end{equation}
	where 
	\begin{equation}
	{\bf A} = \{\phi_{1},\phi_{2},..,\phi_{M}\}
	\end{equation}
	\item The eigenvectors $v_{l}$ of the matrix {\bf L} are determined such that,
	\begin{equation}
	u_{l} = \sum_{k=1}^{M}v_{lk}\phi_{k} 
	\end{equation}
where $l = 1,..,M$.
\end{enumerate}

To recognize a face, the face image is transformed into its eigenface components. The input image $\tau_{new}$ is compared with the mean image and their difference is multiplied with each eigenvector of the {\bf L} matrix. Each value represents a weight and would be saved on a vector $\Omega$.

\begin{equation}
\omega_{k} = u_{k}^{T}(\tau_{new} - \psi)	\Omega^{T} = [\omega_{1},\omega_{2},..,\omega_{k}] 
\end{equation}

The Euclidiean distance $\varepsilon$ is minimized to determine which face class the new face belongs to. It is computed as follows:
\begin{equation}
\varepsilon_{k} = \parallel\Omega - \Omega_{k}\parallel 
\end{equation}

If $\varepsilon_{k}$ is below an established threshold $\theta_{\varepsilon}$, then the input face is considered to belong to that respective class.


\newpage
\section{Software Requirements Specification }
\subsection{ Introduction }

\subsubsection{ Purpose }
The purpose of this Software Requirements Specification is to provide a detailed description of the specifications and functions of the Continuous Authentication system described so far. It is intended as a guide to developers interested in incorporating this model into another system for authentication purpose. This software is developed as a stand-alone system, for demonstration purposes, with the authentication system running in the background acting as a server while the user session is the client. 

\subsubsection{ Scope of the Project }
The Continuous Authentication system aims to ascertain the authenticity of the user's identity throughout the user session. It does so by continuously ensuring that the confidence in the identity of the user doesn't fall below a certain threshold. Throughout the session a video-feed from the web-cam is used to capture the user's traits, which subsequently help in authenticating the user. A session here can be associated with the user logged-in to an e-mail or online banking account, or an account on the operating system itself.

\subsubsection{ Overview of Document }
The rest of this Software Requirements Specification proceeds with a general description of the project followed by the specific requirements of this software. 

\subsection{ General Description }
\subsubsection{ Project Perspective }
The Continuous Authentication system is designed to verify the user's identity at the time of log-in via a password and face recognition and also continually authenticate the user for the entire session. It enters into a lock-down mode in case an unauthorized user attempts to use the authenticated user's privileges.
It demonstrates the authenticating component of a bigger system which manages a user session. An example of such a system could be an instant messaging application. There is a wide range of systems which could incorporate continuous authentication, hence, the interface requirements of this software are currently not defined in detail. Basically, the authentication system should be able to communicate with the parent system whenever an unauthorized user is detected, and either be allowed to act independently and log such a user out or transfer control to the parent system with an appropriate alert.

\subsubsection{ Product Functions }
The Continuous Authentication system aims to provide the following primary functions:
\begin{itemize}
	\item Authenticate the user, at the start of the session, by checking the username-password combination as well as the hard biometric traits (facial features) of the user during the initial log-in stage.
	\item Continually authenticate the user unobtrusively by verifying the user's hard and soft biometric traits based on the confidence level the system generates about the user's authenticity. This includes short time periods during which the user may leave the system.	
	\item Take action (for example, lock-down) when an unauthorized user is detected.
\end{itemize}

\subsubsection{ End users }
This software provides an added layer of security for users who wish to protect access to confidential information or protect their user privileges, in general. Since it is intended to run in the background for authenticating a user based on his/her facial features, the user is not expected to have much technical expertise. They need only know how to operate the parent system of which our Continuous Authentication system can be a component of.

\subsubsection{ General Constraints }
This application is constrained by the following:
\begin{itemize}
	\item The end-user's system should be capable of processing a real-time video-feed.
	\item The lighting conditions during usage should be similar to that captured by the training set for greater accuracy of face recognition.
	\item The system is incapable of performing a hard biometric recognition in case of occlusion.
\end{itemize}

\subsubsection{ Assumptions and Dependencies }
This application is assumes the following:
\begin{itemize}
	\item The user's system is equipped with a web-cam.
	\item The user is working in a sufficiently illuminated environment.
\end{itemize}

\subsection{ Specific Requirements }
\subsubsection{ Functional Requirements }
\begin{itemize}
	\item The system shall allow for creation of an account for a new user in the xml database and collect user's facial features and retrain the model for face recognition.
	\item The system shall provide an initial log-in sequence, and verify user's password followed by the user's hard biometric traits.
	\item If the user is not a part of database, then the system shall exit without logging in the user.
	\item If user is verified at time of log-in, then the system shall continue ascertaining the authenticity of the user's identity via facial recognition and shall log the user in upon successful face recognition, else declare the user unauthorized.
	\item Upon successful log-in, the system shall create a user template with the user's soft biometric traits (shirt colour) once the user's face has been recognized with a minimum given confidence level.
	\item The system shall use the shirt-colour template to verify the user's identity by checking the shirt colour continually till the confidence drops below a specific value.
	\item If the soft biometric verification falls below a certain threshold, the system shall restart face recognition of the user.
	\item If the confidence level indicated by the face recognition system falls below a specific value, it shall assume the user is unauthorized and declare so. %If associated with a parent system it can act as a trigger to initiate a lock-down phase.
	\item If the system doesn't detect a face, it shall not run the face recognition module, and consequently, not run even the shirt-colour detection module. Conversely, if a face is detected, it shall immediately run the hard biometric traits (face recognition) module.
	\item A session time-out shall occur in case the user logs in and then leaves the workstation unattended for a specified time limit.
\end{itemize}

\subsubsection{ Software Requirements }
\begin{itemize}
\item C++ compiler (g++ 4.6.1)
\item OpenCV libraries
\item crypto ++ libraries
\item Drivers for web-cam
\item Python 2.7.1
\end{itemize}

\subsubsection{ Hardware Requirements }
\begin{itemize}
\item Web-cam with drivers supported by OpenCV, with a minimum resolution of 1.3MP
\item Memory : 2GB DDR2 or higher
\item Processor : A minimum of 3.0GhZ single core processor
\end{itemize}

\subsection{ Interface Requirements }
\subsubsection{ User Interface }
The user interface is minimal and consists of the following:
\begin{itemize}
\item Functions necessary to take as input the credentials of the user and display in a window, the live video-feed from the web-cam with details of the user authentication status and the confidence with which the system believes the user is who he/she claims to be.
\item A console window which, if the user is authenticated, processes commands typed by the user, else denies permission. 
\end{itemize}
  
\subsection{ Performance Requirements }
The system is expected to perform without a lag in face detection and recognition on platforms which adhere to the hardware and software requirements mentioned previously.

\section{ System Architecture and Design }  
Our proposed solution can be viewed to exist in one of three following states:
\begin{enumerate}
	\item Conventional password login
	\item Hard biometrics mode
	\item Soft biometrics mode
\end{enumerate}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/overall_f.png}
	\caption{Continuous Authentication system overview}
	\label{fig:ca_overview}
\end{figure}
Each of these states is maintained by designing the respective modules as black-boxes, as seen in figure \ref{fig:ca_overview}.
The control flow moves between these states is based on the confidence of the authenticated user sitting in front of the system.
The modules which implement these states are briefly described in the following sections.

\subsection{Conventional password}
The username-password pair is created during the account creation phase, during which the facial features of the user are also captured.
The password and the (automatically generated) user-id are encrypted and stored in the database.\\
At the time of login, the user is asked to enter the username-password pair, in order to satisfy the condition of \emph{something you know}.
The password entered is encrypted and compared to that stored in the database.
Hence, this phase can be viewed as a gatekeeper to the session. 

\subsection{Hard biometrics}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/hard_f.png}
	\caption{Control flow in Hard Biometrics mode}
	\label{fig:cfhb}
\end{figure}
Once past the successful login phase, the control flow progresses into the hard biometrics mode.
In this phase, the traits unique to the user in front of the system is compared to the parameters obtained during training, i.e, during the account creation phase.
The hard biometric traits captured by our proposed solution is restricted to facial features for two reasons:
\begin{itemize}
	\item To keep the cost of the system low.
	\item In case of more robust biometrics such as iris or fingerprint recognition, the user is required to deviate from the workflow. This hinders the system's ability to capture the required traits without disturbing the user. In contrast, by using facial features, the user can continue working in front of the system oblivious to the authentication process in the background.
\end{itemize}
However, the system is designed in such a way that any other hard biometric traits can be later added in form of a black-box. This requires the machine learning algorithm explained in the next section to be trained accordingly.
The hard biometrics phase hence satisfies the condition - \emph{something the user is}, hence providing a robust method of authenticating the user with high confidence during the session.\\
The facial recognition is achieved using the concept of Eigenfaces\cite{Turk91} as proposed by Turk and Pentland.
Although more complex solutions to face recognition such as Fisherface and neural networks are available, we show that Eigenfaces, even with drawbacks of comparatively lower accuracy and higher time required to process each frame can be overcome using the technique described in the next section.
Once again, the module implementing Eigenfaces can be easily subsituted for a more advanced algorithm, given that it predicts a user from a given image.
But, for the rest of the discussion, the module implementing face recognition implies Eigenface, in order to study a method to overcome the inherent noise from the hard biometrics module.

\subsubsection{Noise dampening}
If the output(confidence) produced by the hard biometrics mode is represented as the indicator function
$$ 1\{user_{recognized} = user_{authenticated}\} \in \{0,1\}$$,
the distribution of these outputs against time appears as shown in the the figure \ref{fig:no_svm}.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.4]{img/no_svm.png}
	\caption{Output of Eigenfaces vs. time as described by the indicator function}
	\label{fig:no_svm}
\end{figure}
The information latent when the false negatives are produced is generally a result of:
\begin{itemize}
	\item A high degree of illumination changes in the surroundings.
	\item Extreme movements and postures by the user, which maps to data not captured during training.
\end{itemize}
Moreover, most face recognition algorithms, including Eigenfaces predicts the closest neighbour in the PCA subspace constructed using the training data.
This leads to a high probability of an imposter being authenticated when the training data is biased, such as in the case of very few registered users or the ratio of male-to-female is not balanced.\\
Our proposed technique overcomes these drawbacks, which applies to face recognition algorithms in general, by taking advantage of:
\begin{itemize}
	\item Temporal information and decaying old information.
	\item Confidence in prediction of recognized face, which in case of Eigenfaces is the Mahalanobis distance of the projected point from its nearest neighbour.
	\item Patterns inherent in the noise.
\end{itemize}
The data extracted for these features from historical records is used to learn a classifier $y \in \{1,-1\}$, implemented as a Support Vector Machine. The outcome of this technique is continued in the results section.

\subsection{Soft biometrics}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/soft_f.png}
	\caption{Control flow in Soft Biometrics mode}
	\label{fig:cfsb}
\end{figure}
Hard biometrics although being more robust and reliable than soft biometrics, falls short in three aspects\cite{Niin10}:
\begin{itemize}
	\item Requires more time for processing each frame.
	\item Restricts the user's postures and movements.
	\item Falsely rejects user due to occlusion.
\end{itemize}
These drawbacks are overcome at the expense of decrease in accuracy to satisfy the goal of not deviating the user from his usual work-flow. 
In our system, the soft biometric trait used is the shirt colour, but is extendable to other soft biometric forms of authentication such as complexion, gender\cite{Jain204}, etc. 


\section{ Detailed Design }
\subsection{ Data Persistence }
The persistent data in this system consists of:
\begin{itemize}
\item Training data set - Training data set consists of 51 equalized, grayscale images of each user stored in their respective directories. These are collected from a web-cam when a new user creates an account.% TO MENTION - resolution of pics
\item Eigenfaces data - This training data set is used to compute, using Principal Component Analysis, a set of "EigenFaces" representing the main differences between the training images. An "average face image" is computed and stored as \verb+avg_image.jpeg+. 
\item User-account data - User-account data refers to user-id, username, password stored in the form of a 160-bit hash string (SHA-1 encrypted).
\end{itemize}

An XML file names \verb+"face_data.xml"+ contains the integrated eigenfaces and user-account data. Its structure is as shown below:
\begin{verbatim}
-<opencv_storage>
	<nPersons>number of users</nPersons>
	<personName_i>username of ith user</personName_i>
	<nEigens>number of eigenfaces</nEigens>
	<nTrainFaces>number of training images</nTrainFaces>
	
	<trainPersonNumMat type_id="opencv-matrix">
		<rows>number of rows</rows>
		<cols>number of columns</cols>
		<dt>data type</dt>
		<data>data</data>
	</trainPersonNumMat>
	<eigenValMat type_id="opencv-matrix">
		<rows>number of rows</rows>
		<cols>number of columns</cols>
		<dt>data type</dt>
		<data>data</data>
	</eigenValMat>
	<projectedTrainFaceMat type_id="opencv-matrix">
		<rows>number of rows</rows>
		<cols>number of columns</cols>
		<dt>data type</dt>
		<data>data</data>
	</projectedTrainFaceMat>
\end{verbatim}

\subsection{ Module Description }
This section describes the various modules of the system which can be categorized as follows:
\begin{itemize}
\item Utilities
\item Face Detection
\item Soft Biometrics
\item Main Module
\end{itemize}

\subsubsection { Utilities }
This module contains all the commonly used image processing functions that perform the following:

\begin{itemize}
\item 
\verb+IplImage* convertImageToGrayscale(IplImage* srcImage)+\\
Convert image to grayscale: Images captured by the webcam need to be converted to grayscale before being processed for computing Eigenfaces. It takes as input an IplImage pointer and returns pointer to the converted grayscale image.
\item
\verb+IplImage* cropImage(IplImage* srcImage, CvRect faceRect)+\\
Return image of the face in the frame defined by a CvRect datatype: Once a face is detected, the rectangular coordinates are saved into a CvRect datatype and the cropped image containing the face is returned to the IplImage pointer.
\item
\begin{verbatim}
IplImage* resizeImage(IplImage* srcImage,
bool preserveAspectRatio = true, int newHeight = _faceHeight,
int newWidth = _faceWidth)
\end{verbatim}
Resize Image: The cropped image containing the face needs to be resized to a standard resolution. This function takes as input source image to be resized, a boolean datatype stating whether to preserve aspect ratio, and the new height and width to be resized to.
\item
\verb+void drawBox(IplImage* image, CvRect rect, CvScalar colour)+\\
Draw a box around the face: This funtion draws a box around the face as indicated by the coordinates it takes as input. This image is later displayed in a window. It also takes a colour value and pointer to the image on which to draw the box.
\end{itemize}

\subsubsection { Face Detection }
This module contains the function that detects a face given an image captured from the webcam. It uses the Viola-Jones Face detection algorithm by making a call to the OpenCV function - cvHaarDetectObjects(). It takes as input the image to detect a face in, and the Haar Classifier Cascade as defined in the respective XML file. This cascade file defines the features of the "object" to look for, in this case, the face. Its prototype is as follows:
\begin{center}
\verb+CvRect detectFace(IplImage* image, CvHaarClassifierCascade* cascade)+
\end{center}

\subsubsection { Soft Biometrics }
This module contains all the functions related to Soft Biometric trait recognition, namely, shirt colour of the user.

\begin{itemize}
\item
\verb+int getPixelColorType(int H, int S, int V);+\\
Obtain the colour type given the Hue (H), saturation(S), and Value(V) values of a pixel.

\item
\begin{verbatim}
map<string, float> createTemplate(CvCapture* capture,
CvHaarClassifierCascade* cascadeFace, int avgIterations)
\end{verbatim}
Create a shirt colour template for the session which contains information about the percentage of each (of 11 specified colour types) colour present in the region detected as shirt region.
\item
\verb+map<string, float>  getTemplate( IplImage*, CvHaarClassifierCascade*, CvRect)+\\
Obtain a session template for a user by detecting a face in the input image and further detecting the shirt region. If region is found the colour composition of the detected shirt frame is calculated and stored in the template.

\item
\verb+map<string, float>  createAverage( vector< map<string, float> > )+\\
Return the average of 5 shirt colour templates created for ascertaining the user's shirt colour. 

\item
\verb+float sigmoid( float )+\\
Calculate the corresponding sigmoid function value given a floating point input. 

\item
\verb+float nrmsd( map<string, float>, map<string, float> )+\\
Calculate the normalized root mean square value given the set of 5 templates and the average template as inputs. 

\item
\verb+int soft_main()+\\
A function that manages the soft biometric traits recogition and is called from the main module.
\end{itemize}

\subsubsection { Main Module }
This module runs in two main phases: Learning and Authentication.

The Learning phase consists of the following major modules:
\begin{itemize}
\item 
\verb+void learn()+\\
This function launches and oversees the learning phase.

\item 
\verb+void doPCA()+\\
This function performs PCA on the loaded training set.

\item 
\verb+void collect()+\\
This function collects the user's face images required for training the face model for recognition. All collected images are processed to give face images in grayscale, equalized format of a standard resized resolution. 

\item 
\verb+int loadFaceImageArr(char* filename)+\\
This function loads the training set in the form of an array for PCA to be performed on it.

\item 
\verb+void storeTrainingData()+\\
This function stores all relevent training data obtained after PCA and Eigenfaces is performed into an XML file.

\item 
\verb+void storeEigenfaceImages()+\\
This function stores all Eigenfaces obtained as an images so they can be checked.

\item 
\verb+string create_hash(string source)+\\
This function creates and returns the hash string using the SHA-1 algorithm taking as input the actual password entered by the user.

\item 
\verb+map<string, string> obtainHashMapFromFile()+\\
This function obtains the hash map of the users and their corresponding hashed password values from the XML file.

\item 
\verb+int appendToFile(map<string, string> hashMap)+\\
This function appends the hash map of usernames and corresponding hashed passwords to the XML file.

\end{itemize}

The Authentication phase consists of the following major modules:
\begin{itemize}

\item 
\verb+string verify_pwd()+\\
This function verifies the password entered by the user by checking its encrypted hash value against the already existing hashed password in the XML file.

\item 
\verb+void spin(string user)+\\
This function launches and oversees the Biometric Authentication phase.
%provide more desc.

\item 
\verb+int loadTrainingData(CvMat** pTrainPersonNumMat)+\\
This function loads the training data, that was stored during learning, for the recognition of the new user attempting to log-in.

\item 
\verb+int recog(IplImage* frame, CvHaarClassifierCascade* faceCascade, CvRect faceRect, CvMat* trainPersonNumMat, float* confidence)+\\
This function carries out the recognition of the new user via the Eigenfaces algorithm.

\item 
\verb+int findNearestNeighbour(float* projectedTestFace, float* confidence)+\\
This function finds the closest face match obtained from Eigenfaces for the given user's face.

\item 
\verb+int loadPersons();+\\
This function loads the data of all persons in the XML database, which includes their username, and user-id.

\end{itemize}


\newpage

\section{ Implementation }  
The continuous authentication system is implemented on the Linux platform, with majority of the code written in C++ and house-keeping tasks of creating and reorganizing training data written in Python 2.7.
Advanced image processing tasks are handled by OpenCV 2.4\cite{opencv} and the machine learning algorithm used is implemented by libSVM\cite{libsvm}.
The system is designed using a client-server model, where the server broadcasts the authentication status of the user.
For the purpose of experimenting and debugging, the information on confidence, logged-in user and the recognized user is displayed on the video feed in a separate window. Using functions integrated within OpenCV, we are able to implement Viola-Jones face detection algorithm and the Eigenface algorithm.

The implementation of the system can be discussed in the following three sections:

\subsubsection{ Username and Password authentication }

The user initially logs in through a conventional username and password using the credentials created by the system administrator. The pseudocode is as follows:

\begin{verbatim}
function login():
    username = get_input()
    time_out = 3
    try = 0
    db_password = retrieve_pwd_for_user(username)
    db_password_hash = SHA1(db_password)
    while try++ < time_out:
        password = get_input()
        password_hash = SHA1(password)
            if password_hash = db_password_hash:
                 return True
    return False
\end{verbatim}

% \begin{figure}[b]
%         \begin{subfigure}[b]{0.6\textwidth}
%                 \centering
%                 \includegraphics[scale=0.4]{img/login.png}
%                 \caption{If password matches, proceed to face recognition}
%                 \label{fig:pwd1}
%         \end{subfigure}%
%         \begin{subfigure}[b]{0.5\textwidth}
%                 \centering
%                 \includegraphics[scale=0.35]{img/login2.png}
%                 \caption{Otherwise, wait till time-out and exit}
%                 \label{fig:pwd2}
%         \end{subfigure}
%         \caption{Conventional username-password login}\label{fig:pwd}
% \end{figure}

\begin{figure}
	\centering
	\subfloat[If password matches, proceed to hard biometrics mode]{%
		\label{fig:pwd1}%
		\includegraphics[scale=0.4]{img/login.png}}
	\quad
	\subfloat[Other wait till time-out and exit]{%
		\label{fig:pwd2}%
		\includegraphics[scale=0.35]{img/login2.png}}
\end{figure}
\subsubsection{ Hard Biometrics Authentication }

Few tasks common to both training and testing of hard biometrics traits are the load-store operations, which transfers data to and from the XML file.
These calls are in the form of \verb+cvRead*(..)+ and \verb+cvWrite*(..)+ which either store or retrieve data in the form of tag-value pairs.
THe proposed system stores a variety of data including encrypted username-password pairs, eigenvectors, average face image, etc. 

The other task predominantly used is the face detection algorithm.
All the processing done in both the hard and soft biometrics mode is with respect to the face image obtained from face detection.
In case of hard biometrics, the requirement for this task is trivial.
But, in the case of soft biometrics, the shirt region is calculated based on the location of the detected face.
In OpenCV, face detection is implemented as \verb+cvHaarDetectObjects(..)+, which can recognize any object defined by the trained Haar-like features.
The trained Haar-like features for facial recognition tasks are provided by default through OpenCV.

Face recognition in OpenCV is implemented by using \verb+cvCalcEigenObjects(..)+ and \verb+cvEigenDecomposite(..)+. 
\verb+cvCalcEigenObjects(..)+ is to called prepare to compute the PCA for the face images in the training dataset.
This is to represent the PCA subspace using an array of eigenvectors.
With the PCA subspace created, the training images needs to be converted to points in this subspace.
OpenCV provides \verb+cvEigenDecomposite(..)+, which projects the given face image to a point in the PCA subspace.

During training, the PCA subspace is created and each face image in the training data is projected into this subspace.
The parameters defining this space and the points are stored in the XML file.
During testing, the PCA subspace is recreated by loading the values and the test image is projected using \verb+cvEigenDecomposite(..)+.
The nearest neighbour to this projected point is then returned as the recognized user.

As discussed earlier, the noise generated by any face recognition algorithm makes the system incapable of handling a continuous stream of bits, each represented by a authenticated/unauthenticated state.
Hence, our proposed solution dampens this noise using a Support Vector Machine with a Gaussian kernel.
The SVM is implemented using C-SVM provided in libSVM\cite{libsvm}.
The features that the model was trained on were:
\begin{itemize}
	\item Mean confidence over $X_{t-T}$ to $X_{t}$ frames
	\item Mean time since authentication over $X_{t-T}$ to $X_{t}$ frames
	\item $\sum_{i=t-T}^{t}\{Recognized\ user = Logged\-in\ user\}$
\end{itemize}
The predicted authentication states are then logged into a bit-vector of size $N$.
The number of bits enabled in this bit-vector dictates the confidence of the system in the hard biometrics mode.
Hence,
\begin{equation}
\theta_{H} = \frac {\textit{No. of bits enabled}} {Length of bit-vector}
\end{equation}

\subsubsection{ Soft biometrics Authentication}

The soft biometrics represented by $\theta_{S}$ is calculated based on the similarity between the current template and the enrolled template.
The template here refers to a vector calculated over a fixed number of frames, whose histogram values have been averaged.
The similarity/confidence is computed using normalized root mean square difference.

The soft biometric trait here is the shirt colour, which is determined by a rectangular region just below the detected face. This is used to determine authentication during the soft biometrics mode. It works as follows:\\[2ex]

\begin{verbatim}
    while(true)
        while(hard_bmt_recog < 0.9)
            frame = get_frame()
            face = preprocess(detect_face(frame))
            hard_bmt_recog = verify(face, face_db)
        // Face recognition gained confidence. Capture soft traits
        soft_template = create_template()
        while(soft_bmt > THRESHOLD)
            soft_details = capture_soft_details()
            soft_bmt = compare(soft_template, soft_details)
\end{verbatim}
% \begin{figure}
% 	\centering
% 		\includegraphics[scale=0.6]{img/soft1.png}
% 	\caption{Soft biometrics - Shirt colour detection}
% \end{figure}

% \begin{figure}[h!]
%         \begin{subfigure}[b]{0.5\textwidth}
%                 \centering
%                 \includegraphics[scale=0.35]{img/soft2.png}
%                 \caption{Face detection run on the capture image and shirt position detected.}
%                 \label{fig:soft2_org}
%         \end{subfigure}%
%         \begin{subfigure}[b]{0.5\textwidth}
%                 \centering
%                 \includegraphics[scale=0.35]{img/soft2_csv.png}
%                 \caption{Frame converted to CSV, and colour composition of the shirt is calculated}
%                 \label{fig:soft2_csv}
%         \end{subfigure}
%         \caption{Authentication using soft biometrics}\label{fig:soft2}
% \end{figure}

\begin{figure}
	\centering
	\subfloat[Face detection runs on the captured image and shirt position is detected.]{%
		\label{fig:pwd1}%
		\includegraphics[scale=0.4]{img/login.png}}
	\quad
	\subfloat[Frame converted to HSV to analyse colour composition]{%
		\label{fig:pwd2}%
		\includegraphics[scale=0.35]{img/login2.png}}
\end{figure}
The soft biometrics template is averaged over frames in $X_{t-k}$ and $X_{t}$ right after the level confidence is highest, that is following the conventional username and password authentication and hard biometrics. Every subsequent k frames are again averaged and compared to the template. 
\begin{figure}
	\centering
	\subfloat[Face detection detects with high accuracy even in natural positions]{%
		\label{fig:soft3_org}%
		\includegraphics[scale=0.35]{img/soft6.png}}
	\quad
	\subfloat[Frame converted to HSV to analyze colour composition]{%
		\label{fig:soft3_csv}%
		\includegraphics[scale=0.35]{img/soft6_csv.png}}
\end{figure}

\begin{figure}
	\centering
	\subfloat[Face detection runs on captured image and new colour composition is enrolled]{%
		\label{fig:soft5_org}%
		\includegraphics[scale=0.4]{img/soft5.png}}
	\quad
	\subfloat[Frame converted to HSV to analyze colour composition]{%
		\label{fig:soft5_csv}%
		\includegraphics[scale=0.35]{img/soft5_csv.png}}
\end{figure}
%$\cdot$\\[10ex]
%$\cdot$\\[10ex]

\newpage
\section{ Testing }

\subsection{ Unit testing }
The two primary units that comprise the Continuous Authentication system were individually tested under real-world conditions.

\subsubsection{ Hard Biometrics - Face Recognition }
\emph{ Requirement: } Faces should be accurately recognized\\
\emph{ Test: } A burst of 200 frames were collected with the user posing naturally with head movement\\
\emph{ Result: } An accuracy of 94\% was achieved.\\

\subsubsection{ Soft biometrics }
\emph{ Requirement: } The soft-biometrics captured should not vary by large from the initial captured template\\
\emph{ Test: } A burst of 200 frames was used with the user performing extreme movements\\
\emph{ Result: } An accuracy of 78\% was achieved\\

\subsection{ Performance testing } 
\emph{ Requirement: } The Continuous Authentication system should be able to meet real-time requirements on a system\\
\emph{ Test: } The code was tested on two systems - one with a Core 2 Duo processor at 3.0Ghz, and the other with a Core i5 processor at 2.66 Ghz\\
\emph{ Result: } The real time requirements was easily met on the Core i5 system, and the Core 2 Duo system, but with a minor lag\\

\subsection{ Security testing}
\emph{ Requirement: } One should not be able to log in without the right credentials, or be able to reverse engineer\\
\emph{ Test: } The passwords stored in the database should not be readable\\
\emph{ Result: } Since the SHA1 hashes of the users were stored in the database, it's harmless even if one tries to retrieve it\\

\subsection{ Compatibility testing }
\emph{ Requirement: } The CA system should be compatible with all the latest versions of the libraries used\\
\emph{ Test: } The code was compiled and executed on g++ 4.5-2.7, OpenCV versions 2.1-2.3 and 2 distributions of Linux\\
\emph{ Result: } The code successfully compiled and executed on all the above versions\\

\subsection{ Load Testing }
\emph{ Requirement: } The CA system should be able to lock-on on one sigle person and perform verification \\
\emph{ Test: } The system was tested with multiple people in the frame\\
\emph{ Result: } A lock-on was always performed on the largest-detected face in the frame\\

\subsection{ Integration testing}
\emph{ Requirement: } By integrating the two main components of Continuous Authentication, the system should successfully switch between them as and when needed\\
\emph{ Test: } A 10 minute run was conducted under real-world conditions\\
\emph{ Result: } The soft-biometrics mode was successfully able to switch to face recognition mode when confidence dropped below a certain threshold\\

\subsection{ System testing }
\emph{ Requirement: } A tail-gating unauthorized user should not be able to gain access for over 3-5 seconds\\
\emph{ Test: } A 10 minute run was conducted with the user and an imposter switching a number of times\\
\emph{ Result: } The imposter was able to gain access for over 5 seconds 2 times out of the 10 switches performed\\


\newpage
\section{ Results }

In this section, we discuss the performance and the design choices made for each module described in the previous sections.
For the rest of the section, the face detection and face recognition algorithm implies Viola-Jones' method and Eigenfaces respectively.

\subsection{Conventional password-based login}
\begin{figure}
	\centering
	\subfloat[Initial prompt]{%
		\label{fig:pwd0}%
		\includegraphics[scale=0.25]{img/ss/pwd0.png}}
	\quad
	\subfloat[Password time-out]{%
		\label{fig:pwd1}%
		\includegraphics[scale=0.25]{img/ss/pwd1.png}}
\end{figure}

At the time of login, the user is prompted to enter his/her username-password pair as show  in figure \ref{fig:pwd0}.
If the username-password entered corresponds to a valid entry in the database, the system enters the hard biometrics mode phase.
A time-out feature as show in figure \ref{fig:pwd1} has been enabled to prevent the user in front of the system resorting to brute force. 

\subsection{Hard Biometrics}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/avg.jpeg}
	\caption{Average image generated by Eigenfaces}
	\label{fig:avg}
\end{figure}
During the Eigenface training phase, for the dataset to be "centered" during PCA, an average image as shown in figure \ref{fig:avg} is computed by calculating the mean of the pixels of all the images in the training data set. 
The faces are then represented as a composition of the average face and a weighted average of the eigenface features as seen in \ref{fig:eigen}.
For example, a person might be characterized as the average plus 20\% from eigenface 1, 12\% from eigenface 2 and so on.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.15]{img/eigen.png}
	\caption{Eigenfaces generated by face recognition algorithm}
	\label{fig:eigen}
\end{figure}
Figure \ref{fig:avg} and \ref{fig:eigen} were generated using 250 images, with each of the 10 users contributing 25 images.
It can be seen that the average image shows a smooth face structure, the first few eigenfaces shows some of the domninant traits and later on it captures mostly noise.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/fd_fr_fdfr.png}
	\caption{Comparison of the facial features processing tasks}
	\label{fig:fdfr}
\end{figure}
Figure \ref{fig:fdfr} was obtained by running the continuous authentication system in a hard biometrics only mode under normal conditions. The average time taken to process each frame for the task of face detection and face recognition are shown in \ref{tab:fdr}

\begin{table}[htp]
	\centering
	\caption{Facial features processing time}
	\begin{tabular}{||l|c||} \hline \hline
			    &  Average time taken \\ \hline
	Face detection      &  0.0224             \\ \hline
	Face recognition    &  0.0454             \\ \hline \hline
	\end{tabular}
	\label{tab:fdr}
\end{table}
In practice, all operations(including face recognition using Eigenfaces) on the image are performed by retrieving the face resized to a fixed height and width.

As mentioned earlier, the accuracy of Eigenfaces is affected by contrasting changes in illumination and posture.
This can be seen in figure \ref{fig:fracc} where the y-axis represents the function $1\{user_{recognized}=user_{authorized}\}$ and UID refers to the User-ID of the respective users in the database.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/face_rec_accuracy.png}
	\caption{Accuracy of face recognition achieved using Eigenfaces}
	\label{fig:fracc}
\end{figure}

\subsection{Soft biometrics}
Figure \ref{fig:fsoft} justifies the reason to transition soft biometrics when necessary.
We observed that after retrieving the face image of the person in front of the system, the soft biometrics consumes half as much time as face recognition.
Note that the time represented for both the observations in figure \ref{fig:fsoft} includes the time taken to pre-process the frame and retrieve the face image.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/face_vs_soft.png}
	\caption{Comparison of Face recognition and Soft biometrics}
	\label{fig:fsoft}
\end{figure}
The average time consumed per frame as observed can be seen in table \ref{tab:frsb}.
As shown, an accuracy of 80\% given that the user was unaware of which colour type the system classified the actual shirt colour into.
Such a condition was assumed since our work provides a solution to extend the soft biometrics to include more modes.
\begin{table}[htp]
	\centering
	\caption{Face recognition vs. Soft biometrics}
	\begin{tabular}{||l|c|c||} \hline \hline
	-                  &  Average time  &  Accuracy \\ \hline
	Face recognition   &  0.12          &  60-80\% \\ \hline
	Soft Biometrics    &  0.045         &  80\% using $\theta_{S}=0.75$ \\ \hline \hline
	\end{tabular}
	\label{tab:frsb}
\end{table}

In figures \ref{fig:soft1} and \ref{fig:softa}, the confidence over time was observed in two cases - when the authenticated user was in front of the system and when he is tailgated by an imposter.
By varying $T$, the template at time $t$ compared the enrolled template can be smoothened out.
This can be seen in both the figures where the peaks are dampened out as a result of increasing $T$.

\begin{figure}
	\centering
	\subfloat[Authenticated user is in front of the system]{%
		\label{fig:soft1}%
		\includegraphics[scale=0.40]{img/soft_conf.png}}
	\quad
	\subfloat[Authenticated user is tailgated]{%
		\label{fig:softa}%
		\includegraphics[scale=0.40]{img/soft_conf_away.png}}
\end{figure}

\subsection{Noise dampening using SVM}
Figure \ref{fig:svm1} captures the output of indicator function as described earlier.
As seen from this figure, the immense noise in the data cannot solely be the basis for predicting if the user in front of the system is the authenticated user.
This noise exists as a result of false positives and false negatives from the prediction. 

\begin{figure}
	\centering
	\subfloat[Output received from the Face recognition module]{%
		\label{fig:svm1}%
		\includegraphics[scale=0.40]{img/no_svm.png}}
	\quad
	\subfloat[Confidence estimated by SVM using Face recognition data]{%
		\label{fig:svm2}%
		\includegraphics[scale=0.40]{img/svm.png}}
\end{figure}

Our system overcomes this by extracting temporal information and the confidence estimated by the face recognition algorithm.
Various situations were modeled and the data extracted was used in training to develop a classifier.
The output produced by this classifier is then represented as confidence, which can be seen in figure \ref{fig:svm}.

In this work, the temporal information is extracted based on a fixed number of previous frames rather than a time period of fixed length.
This is not only because the number of frames processed varies among these time periods as seen in figure \ref{fig:ftime}, but is also dependent on the system's resources and other conditions.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/t_fno.png}
	\caption{Frames processed with respect to time}
	\label{fig:ftime}
\end{figure}

\subsection{Continuous Authentication mode}
\begin{figure}
	\centering
	\subfloat[]{%
		\label{fig:trans1}%
		\includegraphics[scale=0.40]{img/trans_1.png}}
	\quad
	\subfloat[]{%
		\label{fig:trans2}%
		\includegraphics[scale=0.40]{img/trans_2.png}}
\end{figure}
In this subsection, we briefly look into how the control flow jumps between the hard and soft biometrics modules.
Figure \ref{fig:trans1} was plotted by alternating the authenticated user and an imposter in front of the system.
This resulted in a transition from hard to soft biometrics when the system was confident, and the other way round when the system needed to re-inforce its belief.
Figure \ref{fig:trans2} models real-world condition where in the authenticated user after login moves away for a break and an imposter takes his place.
Thus, the soft biometrics mode confidence drops which activates the hard biometrics module.
The predictions made here prove that the authenticated user has been tailgated.

%$\cdot$\\[10ex]
%$\cdot$\\[10ex]
%$\cdot$\\[10ex]
%$\cdot$\\[10ex]
\newpage
\section{ Conclusion and Future Enhancements }
\subsection {Conclusion}
Need for biometric authentication arose from the fact that other security measures such as password and/or a smart ID card are prone to theft or loss. Biometrics depend on utilising features a user already possesses, that can uniquely identify a user atleast for a given session. However, using biometric identification has its downfalls. Since the characteristic of biometric traits is that they are not secretive, unlike a password, they can be gathered anywhere by an imposter and reconstructed for the authentication system to gain access. For example, a fingerprint left behind on some surface, can be picked up by imposters and reconstructed in front of the sensors. Even for face recognition systems, it is possible to fool the system by showing a photograph of the user or manipulating the video feed of the system. Countermeasures exist for such instances, such as, 3d reconstruction of face from more than two cameras or ensuring that the video feed cannot be manipulated. However, these measures make it a costly solution. Therefore, it can be concluded that biometric mode of authentication should be used more as a support system along with password/ smartID systems, to strengthen the authentication process, rather than a standalone mode of authentication.   

\subsection {Future Enhancements}
The following can be implemented in the future to enhance this system:
\begin{itemize}
\item Support for multiple users sharing a certain account; this may require a "biometric handoff" to occur between users.
\item Improve accuracy of face recognition under varied lighting conditions by implementing recognition using a different approach such as fuzzy logic.
\item Make provision for recognizing any kind of tampering occuring to the video feed, so as to prevent authenticating imposters. This can be done by restricting access to the webcam feed via parameters that define access to it.
\end{itemize}

\newpage
\section{ References }
\input{tutref}

\newpage
\section{ Appendix }
\subsection{ Screen Snapshots}
\begin{figure}
	\caption{Occlusion is dealt with by using hard biometrics}
	\centering
\includegraphics[scale=0.3]{img/soft4.png}
\end{figure}

\begin{figure}
	\caption{Unauthorized. Lock-down}
	\centering
\includegraphics[scale=0.3]{img/unauth2.png}
\end{figure}

\begin{figure}
	\caption{Authorized. Authenticated mode.}
	\centering
\includegraphics[scale=0.3]{img/hard2.png}
\end{figure}

\begin{figure}
	\caption{Average image obtained after training. Deviations calculated later from this image to obtain eigenvectors.}
	\centering
\includegraphics[scale=1]{img/avg_image.jpeg}
\end{figure}

\begin{figure}
	\caption{Eigenvectors}
	\centering
\includegraphics[scale=0.2]{img/eigen.png}
\end{figure}
  
\end{document}				% REQUIRED

