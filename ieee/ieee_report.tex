%======================================================================
%----------------------------------------------------------------------
%               XX                              X
%                                               X
%               XX    XXX   XXX   XXX      XXX  X  XXXX
%                X   X   X X   X X   X    X   X X X
%                X   XXXXX XXXXX XXXXX    X     X  XXX
%                X   X     X     X     XX X   X X     X
%               XXX   XXX   XXX   XXX  XX  XXX  X XXXX
%----------------------------------------------------------------------
%  	         A SKELETON FILE FOR IEEE PAPER GENERATION
%----------------------------------------------------------------------
%======================================================================

% first, uncomment the desired options:
\documentclass[%
        %draft,
        submission,
        %compressed,
        %final,
        %
        %technote,
        %internal,
        %submitted,
        %inpress,
        %reprint,
        %
        %titlepage,
        notitlepage,
        %anonymous,
        narroweqnarray,
        inline,
        %twoside,
        ]{ieee}
%
% some standard modes are:
%
% \documentclass[draft,narroweqnarray,inline]{ieee}
% \documentclass[submission,anonymous,narroweqnarray,inline]{ieee}
% \documentclass[final,narroweqnarray,inline]{ieee}

% Use the `endfloat' package to move figures and tables to the end
% of the paper. Useful for `submission' mode.
%\usepackage {endfloat}

% Use the `times' package to use Helvetica and Times-Roman fonts
% instead of the standard Computer Modern fonts. Useful for the 
% IEEE Computer Society transactions.
% (Note: If you have the commercial package `mathtime,' it is much
% better, but the `times' package works too).
%\usepackage {times}

% In order to use the figure-defining commands in ieeefig.sty...
\usepackage{ieeefig}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}

\renewcommand{\thesubfigure}{\thefigure.\arabic{subfigure}}
\captionsetup[subfigure]{labelformat=simple,labelsep=colon,
listofformat=subsimple}
\captionsetup{lofdepth=2}
\makeatletter
\renewcommand{\p@subfigure}{}
\makeatother

\begin{document}

%----------------------------------------------------------------------
% Title Information, Abstract and Keywords
%----------------------------------------------------------------------
\title[Continuous Multimodal User Authentication, using soft and hard biometrics]{%
       Continuous Multimodal User Authentication}

% format author this way for journal articles.
% \author[TRIBHUVANESH, SOUMYA AND K.G.SRINIVASA]{%
%       Tribhuvanesh Orekondy
%       \authorinfo{%
%       Department of Computer Science and Engineering,
%       M.S.Ramaiah Institute of Technology, Bangalore, India
%       Phone: \mbox{}, email: \mbox{tribhuvanesh@gmail.com}}
%     \and
%       Soumya Gosukonda\member{Student}
%       \authorinfo{%
%       Department of Computer Science and Engineering,
%       M.S.Ramaiah Institute of Technology, Bangalore, India
%       Phone: \mbox{}, email: \mbox{soumya.gk@gmail.com}}
%     \and
%       and Dr.K.G.Srinivasa\member{}
%       \authorinfo{%
%       Department of Computer Science and Engineering,
%       M.S.Ramaiah Institute of Technology, Bangalore, India
%       Phone: \mbox{}, email: \mbox{tribhuvanesh@gmail.com}}
%   }

% format author this way for conference proceedings
\author[]{%
      Tribhuvanesh Orekondy,
      \authorinfo{%
      Department of Computer Science and Engineering,\\
      M.S.Ramaiah Institute of Technology, Bangalore, India.\\
      email: tribhuvanesh@gmail.com}
    \and
      Soumya Gosukonda,
      \authorinfo{%
      email: soumya.gk@gmail.com}
    \and
      and Dr. K.G.Srinivasa\member{Member}
      \authorinfo{...}
  }

% specifiy the journal name
% \journal{IEEE Transactions on Something, 1997}

% Or, when the paper is a preprint, try this...
%\journal{IEEE Transactions on Something, 1997, TN\#9999.}

% Or, specify the conference place and date.
% \confplacedate{Ottawa, Canada, May 19--21, 1997}

% make the title
\maketitle               

% do the abstract
\begin{abstract}
Static Authentication provides a rigid and secure framework for a one-time authentication session, but fails to authenticate the user throughout the session.
This leaves the possibility of an imposter gaining access in multiple scenarios.\\
The goal of \emph{Continuous Authentication} is to authenticate the user right from the initial stages of log-in through log-out, which can be implemented by extrapolating the tried-and-tested static authentication techniques thoughout the session.
But, this introduces new challenges, since these "one-time authentication" techniques are computationally expensive, restrict the user's movement and postures in front of the system, require extra expensive hardware and deviate the user from his work-flow.
In these situations, the user no longer remains uninterrupted by the authentication process in the background.\\
This project proposes Continuous Authentication, using login through conventional passwords followed by authentication using two modes - hard and soft biometrics till logout.
The hard biometric trait - facial features, is chosen so that the user need not invest in any additional expensive hardware.
The noise inherent in the process of face recognition, is managed by using a supervised machine learning algorithm which uses output from the facial recognition module to predict if the session has been compromised.
The soft traits are used in phases when this confidence is high to relieve the CPU of comparatively high computation.
\end{abstract}

% do the keywords
\begin{keywords}
Continuous Authentication, Computer Vision, Face recognition, Machine Learning, Support Vector Machine, Biometrics
\end{keywords}

% start the main text ...
%----------------------------------------------------------------------
% SECTION I: Introduction
%----------------------------------------------------------------------
\section{Introduction}
\PARstart
Authentication in the context of computer security is a measure designed to protect the system from fradulent access.
Over the years, this has been achieved by verifying the user's identity and upon successful authentication, granting the user his/her privilieges enabled by a higher authority.
The methods using which a user is authenticated is determined by three \emph{authentication factors}.
Security research has determined that for a positive identification, atleast two of the three authentication factors needs to be satisfied.
These factors are:
\begin{itemize}
	\item {\bf Knowledge factors}: Something the user \emph{knows} (e.g., Username-password pair)
	\item {\bf Ownership factors}: Something the user \emph{has} (e.g., ID card, cell phone, security token)
	\item {\bf Inherence factors}: Something the user \emph{is} (e.g., Fingerprint, retinal patterns, facial features)
\end{itemize}
Klosterman and Granger in their study\cite{Klos00} dilineate authentication based on static and continuous methods.

\emph{Static Authentication} refers to the method of authenticating a user, at the time of log-in.
Password-based authentication are a popular implementation of this concept.
However, passwords can be subjected to being stolen or cracked using various techniques.
Also, static authentication methods, in general, do not ensure that the user remains authenticated throughout the session.
Therefore, there is need for a system that continually verifies the user's identity, while allowing the user to continue his/her work uninterrupted.
Such a system is called a \emph{Continuous Authentication system}.
A lot of research and experimentation has been done in the field of Continuous Authentication \cite{Niin10,Klos00,mon00,turk03,sim07,azz08,azz082}, though it has not been adopted for widespread usage as readily as the Static Authentication systems have been. Continuous Authentication tends to utilize a user's biometric traits for identification, since they can be monitored without distracting the user from his/her work \cite{Klos00}. \emph{Biometric traits} refer to the physiological or behavioural traits of a user that can identify a user, for a session. These traits can be divided into the following two categories:
\begin{itemize}
	\item Hard Biometric traits: These are physical traits of a user that are assumed to be present universally and can uniquely identify an individual. For example, fingerprints, facial features, DNA and so on.
	\item Soft Biometric traits: These are characteristics of a user that "provide some information about the individual, but lack the distinctiveness and permanence to sufficiently differentiate any two individuals"\cite{Jain204}. For example, colour of clothing/skin/eye/hair, gender and other such factors.
\end{itemize}

The issues involved with the integration of biometric-enhanced authentication systems have been well-researched by \emph {Klosterman et al.}\cite{Klos00}. They point out that for unobtrusive continuous monitoring of a user's biometric traits, the trait cannot be something that needs to be in contact with a sensor. Therefore, facial features of a user make for an ideal choice for continuous unobtrusive monitoring. Another important observation in the previously cited paper is the fact that biometrics are expensive to compute. In case of facial features, the image processing and recognition algorithms can be computationally much more expensive as compared to password verification. Therefore, we intend to incorporate Soft Biometric Authentication in our system, which possesses the qualities of being computationally inexpensive and unobtrusive.

The feasibility of using soft biometric traits for user identification was researched by \emph{Jain et al.}\cite{Jain204}, and it was found to reduce computational costs of recognition. A more detailed implementation of authentication incorporating soft biometric traits has been researched into by \emph{Niinuma et al.}\cite{Niin10}. They create a template of the user's shirt and skin colour and generate "similarity scores" for the subsequent frames in which the user is captured. The user is identified based on the comparison of the similarity scores with a certain threshold value. While this model yields fairly good results, according to the results, it leaves scope for improvement of recognition by considering temporal information. By temporal information, we mean, information about the user's identity considered over a certain time period. 
We propose a multimodal continuous system, wherein, a user, upon entering the right username-password combination, will be checked for their facial features (Hard Biometric Authentication). If this is successful then the user is said to be logged in, and then is enrolled for his/her Soft Biometric traits, namely the user's shirt colour. This is continually checked in the subsequent frames, until the user leaves the workstation, or if the shirt colour detected is not the same as that of the user enrolled previously. During Hard Biometric Authentication we intend to recognize the user based on temporal information about the user's identity.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Motivation
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Motivation}
With a gradual shift of enterprise data and even personal account data towards the cloud, almost every individual has cloud presence that is protected by secure access, mostly password-based. But this kind of security does not negate the possibility of tailgating - a situation where a person forgets to log out allowing unauthorized access to their data. It is also cumbersome to log-in multiple times when a user needs to frequently move away from the data access point, as in the case of a medical practitioner accessing patient records and moving away to treat the patient. There is a constant attempt towards making systems more intelligent and in the field of security there is a dire need for an intelligent system that is unobtrusively able to differentiate an authorized user from an unauthorized one without depending solely on password-based methods but also incorporating the recognition of some unique physiological traits a person possesses.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Related Work
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Related Work}
Incorporating Soft Biometric traits for improved accuracy of recognition was first implemented by \emph{A.K.Jain et al} \cite{Jain204}. They proposed a framework for integrating the soft biometric information with the output of the primary biometric system while using fingerprint as the primary biometric identifier and gender, ethnicity, and height as the soft biometric variables. Their results showed 95\% Genuine Acceptance Rate as compared to 90\% in case of using fingerprints alone. 
Using Soft Biometrics for Continuous Authentication has been studied by many researchers where different types of Soft Biometric traits are considered such as keystroke dynamics, electrocardiogram data and colour of clothing and skin.\cite{mon00,ecd,Niin10}. Of these the most relevent is the work by \emph{Niinuma et al.} where they implemented checking the colour of shirt and skin continually to match the template created at the start of the session \cite{Niin10}. They conducted experiments where the user was asked to enact 6 typical scenarios a user may normally exhibit such as turning head in different directions, stretching arms or walking away. They measured the system's performance for soft biometrics evaluation using two main parameters - False Reject and False Accept. Their experimental results indicate an overall False Rejection rate of 4.16\% and a False Accept rate of 0\% over the considered scenarios.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Contribution
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Contribution}
This paper explores the techniques used in various experiments as cited in the previous section.
Furthermore, we look into the implementation aspects of the solution by developing a system using a framework that can be extended to include additioinal hard and soft biometrics modes. 
The modifications in our solution can be summarized as follows:
\begin{itemize}
	\item Hard and soft biometric modes are implemented as black-boxes. The control flow transitions from hard to soft biometric based on confidence $\theta_{H}$ to save the system from additional resources as well as allow the user flexibility in his/her posture. The flow once again enters hard biometric mode based on confidence $\theta_{S}$, which might occur when the user is tailgated.
	\item The hard biometrics predominantly used in this project is the facial features captured using Eigenfaces\cite{Turk91}. More advanced algorithms such as Fisherfaces do exist, but we show that our face recognitioin module can be easily substituted for any of these alternatives. This is achieved by only using value of the indicator function $1\{user_{recognized}=user_{authenticated}\}$.
	\item Most of the face recognition algorithms that have evolved over the years are still striving to achieve an accuracy of 100\%. As show in \cite{fig:no_svm}, Eigenfaces too falls into this category and in our case, this inaccuracy is exhibited in the form of noise. We develop a novel technique to dampen noise using a machine learning algorithm.
	\item Using face recognition algorithms to authenticate faces poses another challenge that the chances of an imposter being missclassified increases when the training set is biased towards a small group of people. We overcome this, once again using machine learning algorithm.
\end{itemize}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Architecture of proposed work
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Architecture of proposed work} \label{sec:arch}
Our proposed solution can be viewed to exist in one of three following states:
\begin{enumerate}
	\item Conventional password login
	\item Hard biometrics mode
	\item Soft biometrics mode
\end{enumerate}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/overall_f.png}
	\caption{Continuous Authentication system overview}
	\label{fig:ca_overview}
\end{figure}
Each of these states is maintained by designing the respective modules as black-boxes, as seen in figure \ref{fig:ca_overview}.
The control flow moves between these states is based on the confidence of the authenticated user sitting in front of the system.
The modules which implementing these states are briefly described in the following sections.

\subsection{Conventional password}
The username-password pair is created during the account creation phase, during which the facial features of the user are also captured.
The password and the (automatically generated) user-id are encrypted and stored in the database.\\
At the time of login, the user is asked to enter the username-password pair, in order to satisfy the condition of \emph{something you know}.
The password entered is encrypted and compared to that stored in the database.
Hence, this phase can be viewed as a gatekeeper to the session. 

\subsection{Hard biometrics}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/hard_f.png}
	\caption{Control flow in Hard Biometrics mode}
	\label{fig:cfhb}
\end{figure}
Once past the successful login phase, the control flow progresses into the hard biometrics mode.
In this phase, the traits unique to the user in front of the system is compared the parameters obtained during training, i.e, during the account creation phase.
The hard biometric traits captured by our proposed solution is restricted to facial features for two reasons:
\begin{itemize}
	\item To keep the cost of the system low.
	\item In case of more robust biometrics such as iris or fingerprint recognition, the user is required to deviate from the workflow. This hinders the system's ability to capture the required traits without disturbing the user. In contrast, by using facial features, the user can continue working in front of the system oblivious to the authentication process in the background.
\end{itemize}
However, the system is designed in such a way that any other hard biometric trait can be later added in form of a black-box. This requires the machine learning algorithm explained in the next section to be trained accordingly.
The hard biometrics phase hence satisfies the condition - \emph{something the user is}, hence providing a robust method of authenticating the user with high confidence during the session.\\
The facial recognition is achieved using the concept of Eigenfaces\cite{Turk91} as proposed by Turk and Pentland.
Although more complex solutions to face recognition such as Fisherface and neural networks are available, we show that Eigenfaces, even with drawbacks of comparatively lower accuracy and higher time required to process each frame can be overcome using the technique described in the next section.
Once again, the module implementing Eigenfaces can be easily subsituted for a more advanced algorithm, given that it predicts a user from a given image.
But, for the rest of the discussion, the module implementing face recognition implies Eigenface, in order to study a method to overcome the inherent noise from the hard biometrics module.

\subsection{Noise dampening}
If the output(confidence) produced by the hard biometrics mode is represented as the indicator function
$$ 1\{user_{recognized} = user_{authenticated}\} \in \{0,1\}$$,
the distribution of these outputs against time appears as shown in the the figure \ref{fig:no_svm}.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.4]{img/no_svm.png}
	\caption{Output of Eigenfaces vs. time as described by the indicator function}
	\label{fig:no_svm}
\end{figure}
The information latent when the false negatives are produced is generally a result of:
\begin{itemize}
	\item A high degree of illumination changes in the surroundings
	\item Extreme movements and postures by the user, which maps to data not captured during training
\end{itemize}
Moreover, most face recognition algorithms, including Eigenfaces predicts the closest neighbour in the PCA subspace constructed using the training data.
This leads to a high probability of an imposter being authenticated when the training data is biased, such as in the case of very few registered users or the ratio of male-to-female is not balanced.\\
Our proposed technique overcomes these drawbacks, which applies to face recognition algorithms in general, by taking advantage of:
\begin{itemize}
	\item Temporal information and decaying old information
	\item Confidence in prediction of recognized face, which in case of Eigenfaces is the Mahalanobis distance of the projected point from its nearest neighbour
	\item Patterns inherent in the noise
\end{itemize}
The data extracted for these features from historical records is used to learn a classifier $y \in \{1,-1\}$, implemented as a Support Vector Machine. The outcome of this technique is continued in the results section.

\subsection{Soft biometrics}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/soft_f.png}
	\caption{Control flow in Soft Biometrics mode}
	\label{fig:cfsb}
\end{figure}
Hard biometrics although being more robust and reliable than soft biometrics, falls short in three aspects\cite{Niin10}:
\begin{itemize}
	\item Required more time for processing each frame
	\item Restricts the user's postures and movements
	\item Falsely rejects user due to occlusion
\end{itemize}
These drawbacks are overcome at the expense of decrease in accuracy to satisfy the goal of not deviating the user from his usual work-flow. 
In our proposed solution, the soft biometric trait used is the shirt colour, but is extendable to other soft biometric forms of authentication such as complexion, gender\cite{Jain204}, etc. 


% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Algorithms and techniques 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Algorithms and techniques used}

\subsection{Face Detection using Viola-Jones algorithm}
The face detector proposed by Paul Viola and Micheal Jones\cite{Viola01} combines four key concepts\cite{servo}:
\begin{itemize}
	\item Simple rectangular features, called Haar features
	\item An Integral Image for rapid feature detection
	\item A variant of the learning algorithm AdaBoost
	\item Cascaded architecture
\end{itemize}
The features that Viola and Jones used are based on Haar wavelets.
Haar wavelets are single wavelength square waves (one high interval and one low interval).
In two dimensions, a square wave is a pair of adjacent rectangles - one light and one dark.
The actual rectangle combinations used for visual object detection are not true Haar wavlets.
Instead, they contain rectangle combinations better suited to visual recognition tasks.
Because of that difference, these features are called Haar features, or Haarlike features, rather than Haar wavelets.
The presence of a Haar feature is determined by subtracting the average dark-region pixel value from the average light-region pixel value.
If the difference is above a threshold (set during learning), that feature is said to be present.\\
To determine the presence or absence of hundreds of Haar features at every image location and at several scales efficiently, Viola and Jones used a technique called an Integral Image.
Using this technique, rectangular features can be evaluated in constant time, which gives them a considerable speed advantage over their more sophisticated relatives.\\
Viola and Jones combined a series of AdaBoost classifiers as a filter chain, that's especially efficient for classifying image regions.
Each filter is a separate AdaBoost classifier with a fairly small number of weak classifiers.
The cascade architecture has interesting implications for the performance of the individual classifiers. Because the activation of each classifier depends entirely on the behavior of its predecessor, the false positive rate for an entire cascade is:
\begin{equation}
F = \prod _{i=1}^{K} f_i
\end{equation}
Similarly, the detection rate is:
\begin{equation}
D = \prod _{i=1}^{K} d_i
\end{equation}
Thus, to match the false positive rates typically achieved by other detectors, each classifier can get away with having surprisingly poor performance. At the same time, however, each classifier needs to be exceptionally capable if it is to achieve adequate detection rates.

\subsection{Face Recognition using Eigenfaces}

\subsubsection{Face recognition using Eigenfaces}
Eigenfaces is a face recognition algorithm that was first described by Turk and Pentland in their paper published in 1991. It works to capture the variations present among the images of faces, that form the training set. It uses this information to create a face model where each image is represented as an eigenvector in what is called a "PCA subspace". Recognition of a test face is carried out by converting the test image into a similar eigenvector and then comparing its distance from all others in the subspace. The person corresponding to closest image is said to be the person recognized in the test image. It encodes the complete face characteristics, as opposed to capturing features of the face separately.\\

\subsubsection{ Eigenface generation }
Eigenfaces works in the following manner:
\begin{enumerate}
	\item A training set of images {\bf S} containing $\tau_{1},\tau_{2},..\tau_{M}$ total faces of the users to be recognized is prepared. These faces are captured during account creation of a user and converted to grayscale and equalized and resized to a standard resolution. Each image is converted to a vector of size {\bf N} by concatenating all the pixels row by row. These vectors are put into a matrix {\bf T} with each row representing an image.
	\item The average of all vectors $\psi$ is calculated and subtracted from each of the vectors in {\bf T} to obtain $\phi_{i}, i = 1,2,..,n$.
	\item The eigenvectors $u_{k}$ and eigenvalues $\lambda_{k}, k = 1,..,M$ of the co-variance matrix {\bf C} are calculated. The covariance matrix itself is found by: 
\begin{equation}
{\bf C} = \frac{1}{M}\sum_{n=1}^{M}\phi_{n}\phi_{n}^{T}
\end{equation}
	\item Since the dimension of C is very high (of the order of the number of pixels in the image), another matrix {\bf L} with the dimensions $MxM$ is constructed, 
	\begin{equation}
	{\bf L} = {\bf A}^{T}{\bf A}
	\end{equation}
	where 
	\begin{equation}
	{\bf A} = \{\phi_{1},\phi_{2},..,\phi_{M}\}
	\end{equation}
	\item The eigenvectors $v_{l}$ of the matrix {\bf L} are determined such that,
	\begin{equation}
	u_{l} = \sum_{k=1}^{M}v_{lk}\phi_{k} 
	\end{equation}
where $l = 1,..,M$.
\end{enumerate}

To recognize a face, the face image is transformed into its eigenface components. The input image $\tau_{new}$ is compared with the mean image and their difference is multiplied with each eigenvector of the {\bf L} matrix. Each value represents a weight and would be saved on a vector $\Omega$.

\begin{equation}
\omega_{k} = u_{k}^{T}(\tau_{new} - \psi)	\Omega^{T} = [\omega_{1},\omega_{2},..,\omega_{k}] 
\end{equation}

The Euclidiean distance $\varepsilon$ is minimized to determine which face class the new face belongs to. It is computed as follows:
\begin{equation}
\varepsilon_{k} = \parallel\Omega - \Omega_{k}\parallel 
\end{equation}

If $\varepsilon_{k}$ is below an established threshold $\theta_{\varepsilon}$, then the input face is considered to belong to that respective class.


\subsection{Noise Dampening using Support Vector Machines}
Using the output produced by the face recogntion module, the goal is to dampen the noise, by taking advantage of certain features like
\begin{itemize}
	\item Temporal information and decaying old information
	\item Confidence in prediction of recognized face, which in case of Eigenfaces is the Mahalanobis distance of the projected point from its nearest neighbour
	\item Patterns inherent in the noise
\end{itemize}
This is achieved by training a classifier on existing data:
\begin{equation}
\mathcal {D} = \{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\ldots,(x^{(m)},y^{(m)})\}
\end{equation}
where $(x^{(i)}, y^{(i)})$ represents the $i^{th}$ training example in a set of $m$ training examples and $x \in \mathbb{R}^n, y \in \{+1, -1\}$. We generate a hyperplane represented as:
\begin{equation}
h_{w,b}(x) = g(w^Tx + b)
\end{equation}
where
\begin{equation}
g(z) = \left\{
\begin{array}{l l}
+1 & \quad \textit{if $z \geq 0$}\\
-1 & \quad \textit{if $z < 0$}\\
\end{array} \right.
\end{equation}
The (primal)optimization problem for finding the optimal margin classifier can be stated as:
\begin{equation}
\min_{\gamma, w, b} \frac{1}{2}\parallel w \parallel ^2\\
\end{equation}
subject to the constraint
\begin{equation}
y^{(i)}(w^Tx + b) \geq i = 1, ..., m
\end{equation}
When we construct the Langrangian for this optimization problem, we have:
\begin{equation} \label{eq:lang}
\mathcal {L}(w, b, \alpha) = \frac{1}{2} \parallel w \parallel ^2 - \sum_{i=1}^{m} \alpha_i [y^{(i)}(w^Tx^{(i)} + b) - 1]
\end{equation}
where $\alpha_{i}$ is a Langrange multiplier. To minimize $\mathcal{L}$, we differentiate this equation with respect to $w$ and $b$ to obtain:
\begin{equation} \label{eq:w}
w = \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}
\end{equation}
\begin{equation} \label{eq:ay}
\sum_{i=1}^{m} \alpha_{i} y^{(i)} = 0
\end{equation}
Plugging this back into equation \ref{eq:lang}, we get
\begin{equation}
\mathcal {L}(w, b, \alpha) = \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i,j=1}^{m} y^{(i)}y^{(j)}\alpha_i \alpha_j (x^{(i)})^T x^{(j)}
\end{equation}
Putting this together with the constraint $\alpha_i \geq 0$, we obtain the following the dual optimization problem:
\begin{equation} \label{eq:dual}
\max_{\alpha} W(\alpha) = \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i,j=1}^{m} y^{(i)}y^{(j)}\alpha_i \alpha_j \langle x^{(i)}, x^{(j)}\rangle
\end{equation}
subject to the constraints
\begin{equation}
\alpha_{i} \geq 0, i = 1,2,..,m
\end{equation}
and
\begin{equation}
\sum_{i=1}^{m} \alpha_{i}y^{(i)} = 0
\end{equation}
By finding the $\alpha$'s given in equation \ref{eq:dual}, which maximizes $W(\alpha)$, the optimal $w$'s can be represented as a function of $\alpha$'s.
Having found $w^{*}$, by considering the primal problem, we obtain the orientation of the hyperplane.
The optimal value of intercept term $b$ can be calculated as:
\begin{equation}
b^{*} = - \frac {max_{i:y^{(i)}=-1}w^{*T}x^{(i)} + min_{i:y^{(i)}=1}w^{*T}x^{(i)}} {2}
\end{equation}

Suppose the model's parameters are fit to the training set, a prediction would require calculate $w^Tx+b$ for a new point $x$. 
This quantity can be written as:
\begin{eqnarray}
w^Tx + b & = & \left( \sum_{i=1}^{m}\alpha_{i} y^{(i)} x^{(i)} \right)^Tx + b \\
         & = & \sum_{i=1}^{m} \alpha_{i}y^{(i)}\langle x^{(i)},x\rangle + b
\end{eqnarray}
Thus, if the value of $\alpha$'s have been calculated, in order to make a prediction a quantity that only depends on the inner product of the new point and training data needs to be calculated.
Let this quantity be represented as $\langle x, z\rangle$.
Given a feature mapping $\phi$, this inner product can be entire replace by $\phi(x),\phi(z)$.
A Kernel can now be defined as:
\begin{equation}
K(x,z) = \phi(x)^T\phi(z)
\end{equation}
In our proposed solution, we use radial basis function as the kernel:
\begin{equation}
K(x,z) = exp\left( \frac {\parallel x - z \parallel ^2} {2\sigma^2} \right)
\end{equation}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Implementation
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Implementation}
The proposed continuous authentication system is implemented on the Linux platform, with majority of the code written in C++ and house-keeping tasks of creating and reorganizing training data written in Python 2.7.
Advanced image processing tasks is handled by OpenCV 2.4\cite{opencv} and the machine learning algorithms used is implemented by libSVM\cite{libsvm}.
The system is designed using a client-server model, where the server broadcasts the authentication status of the user.
For the purpose of experimenting and debugging, the information on confidence, logged-in user and the recognized user is displayed on the video feed in a separate window.
Using functions integrated within OpenCV, we are able to implement Viola-Jones face detection algorithm and the Eigenface algorithm.

Few tasks common to both training and testing of hard biometrics traits are the load-store operations, which transfers data to and from the XML file.
These calls are in the form of \verb+cvRead*(..)+ and \verb+cvWrite*(..)+ which either store or retrieve data in the form of tag-value pairs.
THe proposed system stores a variety of data including encrypted username-password pairs, eigenvectors, average face image, etc. 

The other task predominantly used is the face detection algorithm.
All the processing done in both the hard and soft biometrics mode is with respect to the face image obtained from face detection.
In case of hard biometrics, the requirement for this task is trivial.
But, in the case of soft biometrics, the shirt region is calculated based on the location of the detected face.
In OpenCV, face detection is implemented as \verb+cvHaarDetectObjects(..)+, which can recognize any object defined by the trained Haar-like features.
The trained Haar-like features for facial recognition tasks are provided by default through OpenCV.

Face recognition in OpenCV is implemented by using \verb+cvCalcEigenObjects(..)+ and \verb+cvEigenDecomposite(..)+. 
\verb+cvCalcEigenObjects(..)+ is to called prepare to compute the PCA for the face images in the training dataset.
This is to represent the PCA subspace using an array of eigenvectors.
With the PCA subspace created, the training images needs to be converted to points in this subspace.
OpenCV provides \verb+cvEigenDecomposite(..)+, which projects the given face image to a point in the PCA subspace.

During training, the PCA subspace is created and each face image in the training data is projected into this subspace.
The parameters defining this space and the points are stored in the XML file.
During testing, the PCA subspace is recreated by loading the values and the test image is projected using \verb+cvEigenDecomposite(..)+.
The nearest neighbour to this projected point is then returned as the recognized user.

As discussed earlier, the noise generated by any face recognition algorithm makes the system incapable of handling a continuous stream of bits, each represented by a authenticated/unauthenticated state.
Hence, our proposed solution dampens this noise using a Support Vector Machine with a Gaussian kernel.
The SVM is implemented using C-SVM provided in libSVM\cite{libsvm}.
The features that the model was trained on were:
\begin{itemize}
	\item Mean confidence over $X_{t-T}$ to $X_{t}$ frames
	\item Mean time since authentication over $X_{t-T}$ to $X_{t}$ frames
	\item $\sum_{i=t-T}^{t}\{Recognized\ user = Logged\-in\ user\}$
\end{itemize}
The predicted authentication states are then logged into a bit-vector of size $N$.
The number of bits enabled in this bit-vector dictates the confidence of the system in the hard biometrics mode.
Hence,
\begin{equation}
\theta_{H} = \frac {\textit{No. of bits enabled}} {Length of bit-vector}
\end{equation}

The soft biometrics represented by $\theta_{S}$ is calculated based on the similarity between the current template and the enrolled template.
The template here refers to a vector calculated over a fixed number of frames, whose histogram values have been averaged.
The similarity/confidence is computed using normalized root mean square difference.
 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Results
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Results}
In this section, we study the performance and discuss the design choices made for each module described previously in the section \ref{sec:arch}.
For the rest of the section, the face detection and face recognition algorithm implies Viola-Jones' method and Eigenfaces respectively.

\subsection{Conventional password-based login}
\begin{figure}
	\centering
	\subfloat[Initial prompt]{%
		\label{fig:pwd0}%
		\includegraphics[scale=0.25]{img/ss/pwd0.png}}
	\quad
	\subfloat[Password time-out]{%
		\label{fig:pwd1}%
		\includegraphics[scale=0.25]{img/ss/pwd1.png}}
\end{figure}

At the time of login, the user is prompted to enter his/her username-password pair as show  in figure \ref{fig:pwd0}.
If the username-password entered corresponds to a valid entry in the database, the system enters the hard biometrics mode phase.
A time-out feature as show in figure \ref{fig:pwd1} has been enabled to prevent the user in front of the system resorting to brute force. 

\subsection{Hard Biometrics}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/avg.jpeg}
	\caption{Average image generated by Eigenfaces}
	\label{fig:avg}
\end{figure}
During the Eigenface training phase, for the dataset to be "centered" during PCA, an average image as shown in figure \ref{fig:avg} is computed by calculating the mean of the pixels of all the images in the training data set. 
The faces are then represented as a composition of the average face and a weighted average of the eigenface features as seen in \ref{fig:eigen}.
For example, a person might be characterized as the average plus 20\% from eigenface 1, 12\% from eigenface 2 and so on.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.15]{img/eigen.png}
	\caption{Average image generated by Eigenfaces}
	\label{fig:eigen}
\end{figure}
Figure \ref{fig:avg} and \ref{fig:eigen} were generated using 250 images, with each of the 10 users contributing 25 images.
It can be seen that the average image shows a smooth face structure, the first few eigenfaces shows some of the domninant traits and later on it captures mostly noise.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/fd_fr_fdfr.png}
	\caption{Comparison of the facial features processing tasks}
	\label{fig:fdfr}
\end{figure}
Figure \ref{fig:fdfr} was obtained by running the continuous authentication system in a hard biometrics only mode under normal conditions. The average time taken to process each frame for the task of face detection and face recognition are shown in \ref{tab:fdr}

\begin{table}[htp]
	\centering
	\caption{Facial features processing time}
	\begin{tabular}{||l|c||} \hline \hline
			    &  Average time taken \\ \hline
	Face detection      &  0.0224             \\ \hline
	Face recognition    &  0.0454             \\ \hline \hline
	\end{tabular}
	\label{tab:fdr}
\end{table}
In practise, all operations(including face recognition using Eigenfaces) on the image are performed by retrieving the face resized to a fixed height and width.

As mentioned earlier, the accuracy of Eigenfaces is affected by contrasting changes in illumination and posture.
This can be seen in figure \ref{fig:fracc} where the y-axis represents the function $1\{user_{recognized}=user_{authorized}\}$ and UID refers to the User-ID of the respective users in the database.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/face_rec_accuracy.png}
	\caption{Accuracy of face recognition achieved using Eigenfaces}
	\label{fig:fracc}
\end{figure}

\subsection{Soft biometrics}
Figure \ref{fig:fsoft} justifies the reason to transition soft biometrics when necessary.
We observed that after retrieving the face image of the person in front of the system, the soft biometrics consumes half as much time as face recognition.
Note that the time represented for both the observations in figure \ref{fig:fsoft} includes the time taken to pre-process the frame and retrieve the face image.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/face_vs_soft.png}
	\caption{Comparison of Face recognition and Soft biometrics}
	\label{fig:fsoft}
\end{figure}
The average time consumed per frame as observed can be seen in table \ref{tab:frsb}.
As shown, an accuracy of 80\% given that the user was unaware of the colour of the shirt worn by the user.
Such a condition was assumed since our work provides a solution to extend the soft biometrics to include more modes.
\begin{table}[htp]
	\centering
	\caption{Face recognition vs. Soft biometrics}
	\begin{tabular}{||l|c|c||} \hline \hline
	-                  &  Average time  &  Accuracy \\ \hline
	Face recognition   &  0.12          &  60-80\% \\ \hline
	Soft Biometrics    &  0.045         &  80\% using $\theta_{S}=0.75$ \\ \hline \hline
	\end{tabular}
	\label{tab:frsb}
\end{table}

In figures \ref{fig:soft1} and \ref{fig:softa}, the confidence over time was observed in two cases - when the authenticated user was in front of the system and when he is tailgated by an imposter.
By varying $T$, the template at time $t$ compared the enrolled template can be smoothened out.
This can be seen in both the figures where the peaks are dampened out as a result of increasing $T$.

\begin{figure}
	\centering
	\subfloat[Authenticated user is in front of the system]{%
		\label{fig:soft1}%
		\includegraphics[scale=0.40]{img/soft_conf.png}}
	\quad
	\subfloat[Authenticated user is tailgated]{%
		\label{fig:softa}%
		\includegraphics[scale=0.40]{img/soft_conf_away.png}}
\end{figure}

\subsection{Noise dampening using SVM}
Figure \ref{fig:svm1} captures the output of indicator function as described earlier.
As seen from this figure, the immense noise in the data cannot solely be the basis for predicting if the user in front of the system is the authenticated user.
This noise exists as a result of false positives and false negatives from the prediction. 

\begin{figure}
	\centering
	\subfloat[Output received from the Face recognition module]{%
		\label{fig:svm1}%
		\includegraphics[scale=0.40]{img/no_svm.png}}
	\quad
	\subfloat[Confidence estimated by SVM using Face recognition data]{%
		\label{fig:svm2}%
		\includegraphics[scale=0.40]{img/svm.png}}
\end{figure}

Our solution overcomes this by extracting temporal information and the confidence estimated by the face recognition algorithm.
Various situations were modeled and the data extracted was used in training to develop a classifier.
The output produced by this classifier is then represented as confidence, which can be seen in figure \ref{fig:svm2}.

In this work, the temporal information is extracted based on a fixed number of previous frames rather than a time period of fixed length.
This is not only because the number of frames processed varies among these time periods as seen in figure \ref{fig:ftime}, but is also dependent on the system's resources and other conditions.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/t_fno.png}
	\caption{Frames processed with respect to time}
	\label{fig:ftime}
\end{figure}

\subsection{Continuous Authentication mode}
\begin{figure}
	\centering
	\subfloat[]{%
		\label{fig:trans1}%
		\includegraphics[scale=0.40]{img/trans_1.png}}
	\quad
	\subfloat[]{%
		\label{fig:trans2}%
		\includegraphics[scale=0.40]{img/trans_2.png}}
\end{figure}
In this subsection, we briefly look into how the control flow jumps between the hard and soft biometrics modules.
Figure \ref{fig:trans1} was plotted by alternating the authenticated user and an imposter in front of the system.
This resulted in a transition from hard to soft biometrics when the system was confident, and the other way round when the system needed to re-inforce its belief.
Figure \ref{fig:trans2} models real-world condition where in the authenticated user after login moves away for a break and an imposter takes his place.
Thus, the soft biometrics mode confidence drops which activates the hard biometrics module.
The predictions made here prove that the authenticated user has been tailgated.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Conclusion and Future enhancements
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Conclusion and Future enhancements}
\noindent{}Need for biometric authentication arose from the fact that other security measures such as password and/or a smart ID card are prone to theft or loss. Biometrics depend on utilising features a user already possesses, that can uniquely identify a user atleast for a given session. However, using biometric identification has its downfalls. Since the characteristic of biometric traits is that they are not secretive, unlike a password, they can be gathered anywhere by an imposter and reconstructed for the authentication system to gain access. For example, a fingerprint left behind on some surface, can be picked up by imposters and reconstructed in front of the sensors. Even for face recognition systems, it is possible to fool the system by showing a photograph of the user or manipulating the video feed of the system. Countermeasures exist for such instances, such as, 3d reconstruction of face from more than two cameras or ensuring that the video feed cannot be manipulated. However, these measures make it a costly solution. Therefore, it can be concluded that biometric mode of authentication should be used more as a support system along with password/ smartID systems, to strengthen the authentication process, rather than a standalone mode of authentication.   

\noindent{}The following can be implemented in the future to enhance this system:
\begin{itemize}
\item Support for multiple users sharing a certain account; this may require a "biometric handoff" to occur between users.
\item Improve accuracy of face recognition under varied lighting conditions by implementing recognition using a different approach such as fuzzy logic.
\item Make provision for recognizing any kind of tampering occuring to the video feed, so as to prevent authenticating imposters. This can be done by restricting access to the webcam feed via parameters that define access to it.
\end{itemize}
% do the biliography:
\bibliographystyle{IEEEbib}
\input{tutref}

\end{document}
