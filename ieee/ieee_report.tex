%======================================================================
%----------------------------------------------------------------------
%               XX                              X
%                                               X
%               XX    XXX   XXX   XXX      XXX  X  XXXX
%                X   X   X X   X X   X    X   X X X
%                X   XXXXX XXXXX XXXXX    X     X  XXX
%                X   X     X     X     XX X   X X     X
%               XXX   XXX   XXX   XXX  XX  XXX  X XXXX
%----------------------------------------------------------------------
%  	         A SKELETON FILE FOR IEEE PAPER GENERATION
%----------------------------------------------------------------------
%======================================================================

% first, uncomment the desired options:
\documentclass[%
        %draft,
        %submission,
        %compressed,
        final,
        %
        %technote,
        %internal,
        %submitted,
        %inpress,
        %reprint,
        %
        %titlepage,
        notitlepage,
        %anonymous,
        narroweqnarray,
        inline,
        %twoside,
        ]{ieee}
%
% some standard modes are:
%
% \documentclass[draft,narroweqnarray,inline]{ieee}
% \documentclass[submission,anonymous,narroweqnarray,inline]{ieee}
% \documentclass[final,narroweqnarray,inline]{ieee}

% Use the `endfloat' package to move figures and tables to the end
% of the paper. Useful for `submission' mode.
%\usepackage {endfloat}

% Use the `times' package to use Helvetica and Times-Roman fonts
% instead of the standard Computer Modern fonts. Useful for the 
% IEEE Computer Society transactions.
% (Note: If you have the commercial package `mathtime,' it is much
% better, but the `times' package works too).
%\usepackage {times}

% In order to use the figure-defining commands in ieeefig.sty...
\usepackage{ieeefig}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}

\renewcommand{\thesubfigure}{\thefigure.\arabic{subfigure}}
\captionsetup[subfigure]{labelformat=simple,labelsep=colon,
listofformat=subsimple}
\captionsetup{lofdepth=2}
\makeatletter
\renewcommand{\p@subfigure}{}
\makeatother

\begin{document}

%----------------------------------------------------------------------
% Title Information, Abstract and Keywords
%----------------------------------------------------------------------
\title[Continuous Multimodal User Authentication]{%
       Continuous Multimodal User Authentication: coupling hard and soft biometrics with Support Vector Machines to attenuate Noise}

% format author this way for journal articles.
\author[]{%
      Tribhuvanesh Orekondy
      \authorinfo{%
      Department of Computer Science and Engineering,
      M.S.Ramaiah Institute of Technology, Bangalore, India\\
      email: \mbox{tribhuvanesh@gmail.com}}
    \and
      Soumya Gosukonda
      \authorinfo{%
      Department of Computer Science and Engineering,
      M.S.Ramaiah Institute of Technology, Bangalore, India\\
      email: \mbox{soumya.gk@gmail.com}}
    \and
      and  K.G.Srinivasa\member{Member}
      \authorinfo{%
      Department of Computer Science and Engineering,
      M.S.Ramaiah Institute of Technology, Bangalore, India\\
      email: \mbox{kgsrinivas@msrit.edu}}
  }

% format author this way for conference proceedings
% \author[]{%
%       Tribhuvanesh Orekondy,
%       \authorinfo{%
%       Department of Computer Science and Engineering,\\
%       M.S.Ramaiah Institute of Technology, Bangalore, India.\\
%       email: tribhuvanesh@gmail.com}
%     \and
%       Soumya Gosukonda,
%       \authorinfo{%
%       email: soumya.gk@gmail.com}
%     \and
%       and Dr. K.G.Srinivasa\member{Member}
%       \authorinfo{...}
%   }

% specifiy the journal name
% \journal{IEEE Transactions on Something, 1997}

% Or, when the paper is a preprint, try this...
%\journal{IEEE Transactions on Something, 1997, TN\#9999.}

% Or, specify the conference place and date.
% \confplacedate{Ottawa, Canada, May 19--21, 1997}

% make the title
\maketitle               

% do the abstract
\begin{abstract}
Static Authentication provides a rigid and secure framework for a one-time authentication session, but fails to authenticate the user throughout the session.
This leaves the possibility of an imposter gaining access in multiple scenarios.

The goal of \emph{Continuous Authentication} is to authenticate the user right from the initial stages of log-in through log-out, which can be implemented by extrapolating the tried-and-tested static authentication techniques thoughout the session.
But, this introduces new challenges, since these "one-time authentication" techniques are computationally expensive, restrict the user's movement and postures in front of the system, require extra expensive hardware and deviate the user from his work-flow.
In these situations, the user no longer remains uninterrupted by the authentication process in the background.

This project proposes Continuous Authentication, using login through conventional passwords followed by authentication using two modes - hard and soft biometrics till logout.
The hard biometric trait - facial features, is chosen so that the user need not invest in any additional expensive hardware.
The noise inherent in the process of face recognition, is managed by using a supervised machine learning algorithm which uses output from the facial recognition module to predict if the session has been compromised.
The soft traits are used in phases when this confidence is high to relieve the CPU of comparatively high computation.
\end{abstract}

% do the keywords
\begin{keywords}
Continuous Authentication, Computer Vision, Face recognition, Machine Learning, Support Vector Machine, Biometrics
\end{keywords}

% start the main text ...
%----------------------------------------------------------------------
% SECTION I: Introduction
%----------------------------------------------------------------------
\section{Introduction}
\PARstart
Authentication in the context of computer security is a process which verifies the claimed identity of the user.
Upon successfule authentication, the user is granted privilieges enabled by a higher authority.
A number of elements together can decide the authenticity of the user before authorizing him/her.
These elements is classified based on three factors called \emph{authentication factors}.
Security research has determined that for a positive identification, atleast two of the three authentication factors needs to be satisfied.
These factors are:

\begin{itemize}
	\item \textbf{Knowledge factors}: Something the user \emph{knows} (e.g., Username-password pair)
	\item \textbf{Ownership factors}: Something the user \emph{has} (e.g., ID card, cell phone, security token)
	\item \textbf{Inherence factors}: Something the user \emph{is} (e.g., Fingerprint, retinal patterns, facial features)
\end{itemize}

The purpose of this work is to extend the conventional user authentication techniques using these factors in an unobtrusive way.

This leads for the need to delineate the concepts of \emph{static} and \emph{continuous} authentication.
%Klosterman and Granger in their study\cite{Klos00} dilineate authentication based on static and continuous methods.
\emph{Static Authentication} refers to the method of authenticating a user, at the time of log-in.
In most cases, knowledge-based methods such as passwords are used since every user can verify his claim in a very convinient manner.
But, passwords and knowledge factors lose their credibility when shared, forgotten or stolen.
Similarly, ownernership-based methods too can duplicated, stolen or lost.
When dealing with sensitive content one might resort to additional equipment to verify the user based on their unique traits such as fingerprints or retinal patterns.
However, in this case, the only period during which system is very confident and fully aware of user's identity claim is during the authentication period.
For example, computer setups nowadays use a 2-factor authentication technique by extracting the password and the user's facial features at the time of login and assumes it is the same authenticated user till logout.
But when the user moves away from the system to take a break without logging-out, it is susceptible to tailgating when an imposter takes the authenticated user's place.
This could prove to be a critical security weakness in  high-security systems.
In order to address this issue, the system needs to continuously verify the user's identity claim in an unobtrusive manner.

The concept of \emph{Continuous Authentication} initally studied by Klosterman et al.\cite{Klos00}, showed an elegant approach of using biometrics to verify the user's identity through-out the session.
Not all modes of "biometrics" can achieve this goal since methods like finger-prints and retinal scans distracts the user and he/she no longer remains uniterrupted by the authentication process in the background.
A lot of research and experimentation has been carried out in the field of Continuous Authentication \cite{Niin10,Klos00,mon00,turk03,sim07,azz08,azz082}, though it has not been adopted for widespread usage as readily as the Static Authentication systems have been mainly due to convenience.

Before proceeding to explain the challenges the development of a Continuous Authentication system poses, an overview of \emph{Biometric traits} in the context of Continuous Authentication is necessary.
\emph{Biometric traits} refer to the physiological or behavioural traits of a user that can identify a user, for a session.
These traits can be divided into the following two categories:
\begin{itemize}
	\item {\bf Hard Biometric traits}: Physical traits of a user that are assumed to be present universally and can uniquely identify an individual. For example, fingerprints, facial features, DNA and so on.
	\item {\bf Soft Biometric traits}: These are characteristics of a user that "provide some information about the individual, but lack the distinctiveness and permanence to sufficiently differentiate any two individuals"\cite{Jain204}. For example, colour of clothing/skin/eye/hair, gender and other such factors.

\end{itemize}

The study conducted by Klosterman et al.\cite{Klos00} in the design of biometric-enhanced authentication system also describes the challenges posed by such a system.
They point out that for unobtrusive continuous monitoring of a user's biometric traits, the trait cannot be something that needs to be in contact with a sensor.
Therefore, facial features of a user make for an ideal choice for continuous unobtrusive monitoring.
Another important observation in the previously cited paper is the fact that biometrics are expensive to compute.
In case of facial features, the image processing and recognition algorithms can be computationally much more expensive as compared to password verification.
Therefore, we intend to incorporate Soft Biometric Authentication in our system, which is computationally inexpensive and also allows the user more flexibilty in posture. 

The feasibility of using soft biometric traits for user identification was researched by \emph{Jain et al.}\cite{Jain204}, and it was found to reduce computational costs of recognition.
A more detailed implementation of authentication incorporating soft biometric traits has been researched by \emph{Niinuma et al.}\cite{Niin10}.
They create a template of the user's shirt and skin colour and generate "similarity scores" for the subsequent frames in which the user is captured.
The user is identified based on the comparison of the similarity scores with a certain threshold value.
While this model yields fairly good results, according to the results, it leaves scope for improvement of recognition by considering temporal information.
By temporal information, we mean a technique similar to a sliding window which analyzes patterns around the time the authentication state is to be predicted.
Hence, this implies a decay of data beyond a certain time in the past.

We address this problem by developing a Continuous Authentication system, wherein, a user at the time of login initiates a session by entering the right username-password combination.
Upon successful login, provided the user's identity is valid, the system proceeds into a hard biometrics mode and subsequently a soft biometrics mode.
This work incorporates the use of facial features and the colour of the user's clothing in these two modes repectively. 
Furthermore, a supervised machine learning algorithm has been used to bridge the gap between the two modes.
This enables the system to use any face recognition algorithm which produces the desired output as explained in the later sections.
The temporal information suggested earlier is used by this machine learning algorithm to predict the authentication state at any given point in time.
When the confidence of the system in the hard biometrics mode is beyond the specified threshold, the system enrolls the soft biometric template of the user and transitions into the soft biometrics mode.

The rest of this paper is organized as follows:
Section \ref{sec:motivation} presents our motivating case to employ a Continuous Authentication system,
Section \ref{sec:related} describes related work conducted in this field,
Section \ref{sec:contribution} discusses how we extend the work and the novel contributions developed in the field of continuous authentication,
Section \ref{sec:arch} introduces the architecture the proposed Continuous Authentication system uses,
Section \ref{sec:algo} provides an insight to the algorithms used in various modules,
Section \ref{sec:implementation} looks into the implementation aspect of the work,
Section \ref{sec:results} presents experimental results and discussion and
Section \ref{sec:conclusion} provides conclusions and future work

% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Motivation
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Motivation} \label{sec:motivation}
With a gradual shift of enterprise data and even personal account data towards the cloud, almost every individual has cloud presence that is protected by secure access, mostly password-based.
As suggested in the 2-factor static authentication example, this kind of security does not address the possibility of tailgating - a situation where a person forgets to log out allowing unauthorized access to their data.
Moreover, the online marketplaces on mobile devices as well as desktops today come enabled with a pre-authorized account where the user is not prompted for any credententials before making purchases.
Hence, the convenience for the user is achieved at the cost of a possibility of an unauthorized person making fradulent purchases.

A possible solution to this is using the biometrics characterizing the user thoughout the session.
But, these conventional "biometrics" techniques have low availability. 
Advanced methods such as fingerprint and retinal pattern recognition proves very inconvenient to user since the posture would be restricted.
Facial features too restrict the user to face the camera and features not captures in the training data surfaces as noise during practise.

It is also cumbersome to log-in multiple times when a user needs to frequently move away from the data access point, as in the case of a medical practitioner accessing patient records and moving away to treat the patient.
There is a constant attempt towards making systems more intelligent and in the field of security there is a dire need for an intelligent system that is unobtrusively able to differentiate an authorized user from an unauthorized one without depending solely on password-based methods but also incorporating the recognition of some unique physiological traits a person possesses.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Related Work
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Related Work} \label{sec:related}
Incorporating Soft Biometric traits for improved accuracy of recognition was first implemented by \emph{A.K.Jain et al} \cite{Jain204}.
They proposed a framework for integrating the soft biometric information with the output of the primary biometric system while using fingerprint as the primary biometric identifier and gender, ethnicity, and height as the soft biometric variables.
Their results showed 95\% Genuine Acceptance Rate as compared to 90\% in case of using fingerprints alone. 

Using Soft Biometrics for Continuous Authentication has been studied by many researchers where different types of Soft Biometric traits are considered such as keystroke dynamics, electrocardiogram data and colour of clothing and skin.\cite{mon00,ecd,Niin10}.
Of these the most relevent is the work by \emph{Niinuma et al.} where they implemented checking the colour of shirt and skin continually to match the template created at the start of the session \cite{Niin10}.
They conducted experiments where the user was asked to enact 6 typical scenarios a user may normally exhibit such as turning head in different directions, stretching arms or walking away.
A measure on the system's performance for soft biometrics was made by evaluating two main parameters - False Reject and False Accept.
Their experimental results indicate an overall False Rejection rate of 4.16\% and a False Accept rate of 0\% over the considered scenarios.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Contribution
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Contribution} \label{sec:contribution}
This paper explores the techniques used in various experiments as cited in the previous section.
Furthermore, we look into the implementation aspects of the solution by developing a system using a framework that can be extended to include additioinal hard and soft biometrics modes. 
Our approach to advance the field of Continuous Authentication can summarized as being made in the following fields:
\begin{itemize}
	\item Hard and soft biometric modes are implemented as black-boxes. The control flow transitions from hard to soft biometric based on confidence $\theta_{H}$ to save the system from additional resources as well as allow the user flexibility in his/her posture. The flow once again enters hard biometric mode based on confidence $\theta_{S}$, which might occur when the user is tailgated.
	\item The hard biometrics predominantly used in this project is the facial features captured using Eigenfaces\ref{Turk91}. More advanced algorithms such as Fisherfaces do exist, but we show that our face recognition module can be easily substituted for any of these alternatives. This is achieved by using the features common among face recognition algorithms - the value of indicator function $1\{user_{recognized}=user_{authenticated}\}$ and a confidence measure.
	\item Most of the face recognition algorithms that have evolved over the years are still striving to achieve a perfect accuracy. As show in \cite{fig:no_svm}, Eigenfaces too falls into this category and in our case, this inaccuracy is exhibited in the form of noise. We develop a novel technique to dampen noise using a machine learning algorithm.
	\item Using face recognition algorithms to authenticate faces poses another challenge that the chances of an imposter being missclassified increases when the training set is biased towards a small group of people. We overcome this, once again using machine learning algorithm.
\end{itemize}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Architecture of proposed work
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Architecture of proposed work} \label{sec:arch}
The control flow in our proposed solution can be viewed to exist in one of three following states:
\begin{enumerate}
	\item Conventional password login
	\item Hard biometrics mode
	\item Soft biometrics mode
\end{enumerate}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/overall_f.png}
	\caption{Continuous Authentication system overview}
	\label{fig:ca_overview}
\end{figure}
Each of these states is maintained by designing the respective modules as black-boxes, as seen in figure \ref{fig:ca_overview}.
The control flow moves between these states is based on the confidence of the authenticated user sitting in front of the system.
The modules which implementing these states are briefly described in the following sections.

\subsection{Conventional password}
The username-password pair is created in the account creation mode, during which the facial features of the user are also captured.
The password and the (automatically generated) user-id are encrypted and stored in the database.\\
At the time of login, the user is asked to enter the username-password pair, in order to satisfy the condition of \emph{something you know}.
The password entered is encrypted and compared to that stored in the database.
Hence, this phase can be viewed as a gatekeeper to the session. 

\subsection{Hard biometrics}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/hard_f.png}
	\caption{Control flow in Hard Biometrics mode}
	\label{fig:cfhb}
\end{figure}
Once past the successful login phase, the control flow progresses into the hard biometrics mode.
In this phase, the traits unique to the user in front of the system is compared the parameters obtained during training, i.e, during the account creation phase.
The hard biometric traits captured by our proposed solution is restricted to facial features for two reasons:
\begin{itemize}
	\item To keep the cost of the system low.
	\item In case of more robust biometrics such as iris or fingerprint recognition, the user is required to deviate from the workflow. This hinders the system's ability to capture the required traits without disturbing the user. In contrast, by using facial features, the user can continue working in front of the system oblivious to the authentication process in the background.
\end{itemize}
However, the system is designed in such a way that any other hard biometric trait can be later added in form of a black-box. This requires the machine learning algorithm explained in the next section to be trained accordingly.
The hard biometrics phase hence satisfies the condition - \emph{something the user is}, hence providing a robust method of authenticating the user with high confidence during the session.\\
The facial recognition used is Eigenfaces\cite{Turk91} as proposed by Turk and Pentland.
Although more complex solutions to face recognition such as Fisherface and neural networks are available, we show that Eigenfaces, even with drawbacks of comparatively lower accuracy and higher time required to process each frame can be overcome using the technique described in the next section.
This module implementing Eigenfaces can be easily subsituted for a more advanced algorithm, given that it predicts a user from a given image.
But, for the rest of the discussion, the module implementing face recognition implies Eigenface, in order to study a method to overcome the inherent noise from the hard biometrics module.

\subsection{Noise dampening}
If the output(confidence) produced by the hard biometrics mode is represented as the indicator function
\begin{equation}
1\{user_{recognized} = user_{authenticated}\} \in \{0,1\}
\end{equation}
the distribution of these outputs against time appears as shown in the the figure \ref{fig:no_svm}.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.4]{img/no_svm.png}
	\caption{Output of Eigenfaces vs. time as described by the indicator function}
	\label{fig:no_svm}
\end{figure}
The information latent in the noise - the false positives and negatives, is produced as a result of:
\begin{itemize}
	\item A high degree of illumination changes in the surroundings
	\item Extreme movements and postures by the user, which maps to data not captured during training
\end{itemize}
Moreover, most face recognition algorithms, including Eigenfaces predicts the closest neighbour to the projected point in the PCA subspace, which was constructed during training.
This leads to a high probability of an imposter being authenticated under conditions when the training data is biased, such as in the case of very few registered users or the ratio of male-to-female is not balanced.\\
For example, this might occur when the database is new and consists of a single user. 
In this case, Eigenfaces recognizes every face as this user 
(In section \ref{sec:results} we show that our noise dampening technique can overcome this flaw.)
Our proposed technique overcomes these drawbacks, which applies to face recognition algorithms in general, by taking advantage of:
\begin{itemize}
	\item Temporal information. We use a sliding-window technique which uses data from past $T$ frames.
	\item Confidence in prediction of recognized face, which in case of Eigenfaces is the Mahalanobis distance of the projected point from its nearest neighbour
	\item Patterns inherent in the noise
\end{itemize}
The data extracted for these features from historical records is used to learn a classifier $y \in \{1,-1\}$, implemented as a Support Vector Machine. The outcome of this technique is continued in the results section.

\subsection{Soft biometrics}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/soft_f.png}
	\caption{Control flow in Soft Biometrics mode}
	\label{fig:cfsb}
\end{figure}
Hard biometrics although being more robust and reliable than soft biometrics, falls short in three aspects\cite{Niin10}:
\begin{itemize}
	\item Requires more time for processing each frame
	\item Restricts the user's postures and movements
	\item Falsely rejects user due to occlusion and contrasting changes in facial expression.
\end{itemize}
These drawbacks are overcome at the expense of decrease in accuracy to satisfy the goal of not deviating the user from his usual work-flow. 
In our proposed solution, the soft biometric trait used is by enrolling a HSV template of the colour of user's clothing, but can be extended to include other soft biometric forms of authentication such as complexion, gender\cite{Jain204}, etc. 


% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Algorithms and techniques 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Algorithms and techniques used} \label{sec:algo}

\subsection{Face Detection using Viola-Jones algorithm}
The face detector proposed by Paul Viola and Micheal Jones\cite{Viola01} combines four key concepts\cite{servo}:
\begin{itemize}
	\item Simple rectangular features, called Haar features
	\item An Integral Image for rapid feature detection
	\item A variant of the learning algorithm AdaBoost
	\item Cascaded architecture
\end{itemize}
The features that Viola and Jones used are based on Haar wavelets.
Haar wavelets are single wavelength square waves (one high interval and one low interval).
In two dimensions, a square wave is a pair of adjacent rectangles - one light and one dark.
The actual rectangle combinations used for visual object detection are not true Haar wavlets.
Instead, they contain rectangle combinations better suited to visual recognition tasks.
Because of that difference, these features are called Haar features, or \emph{Haarlike features}, rather than Haar wavelets.
The presence of a Haar feature is determined by subtracting the average dark-region pixel value from the average light-region pixel value.
If the difference is above a threshold (set during learning), that feature is said to be present.\\
To determine the presence or absence of hundreds of Haar features at every image location and at several scales efficiently, Viola and Jones used a technique called an \emph{Integral Image}.
Using this technique, rectangular features can be evaluated in constant time, which gives them a considerable speed advantage over their more sophisticated relatives.\\
Viola and Jones combined a series of \emph{AdaBoost classifiers} as a filter chain, that's especially efficient for classifying image regions.
Each filter is a separate AdaBoost classifier with a fairly small number of weak classifiers.
The cascade architecture has interesting implications for the performance of the individual classifiers. Because the activation of each classifier depends entirely on the behavior of its predecessor, the false positive rate for an entire cascade is:
\begin{equation}
F = \prod _{i=1}^{K} f_i
\end{equation}
Similarly, the detection rate is:
\begin{equation}
D = \prod _{i=1}^{K} d_i
\end{equation}
Thus, to match the false positive rates typically achieved by other detectors, each classifier can get away with having surprisingly poor performance. At the same time, however, each classifier needs to be exceptionally capable if it is to achieve adequate detection rates.

\subsection{Face Recognition using Eigenfaces}

\subsubsection{Face recognition using Eigenfaces}
Eigenfaces is a face recognition algorithm that was first described by Turk and Pentland\cite{Turk91}. It works to capture the variations present among the images of faces, that form the training set. It uses this information to create a face model where each image is represented as an eigenvector in what is called a PCA subspace. Recognition of a test face is carried out by converting the test image into a similar eigenvector and then comparing its distance from all others in the subspace. The person corresponding to closest image is said to be the person recognized in the test image. It encodes the complete face characteristics, as opposed to capturing features of the face separately.\\

\subsubsection{ Eigenface generation }
Eigenfaces are generated in the following manner, as described in \cite{Turk91}:
\begin{enumerate}
	\item During the account creation phase, a series of face images are captured and preprocessed. Hence a training dataset {\bf S} of preprocessed face images containing $\tau_{1},\tau_{2},..\tau_{M}$ is prepared. Each image is then converted to a vector of size {\bf N} by concatenating all the pixels row by row. These vectors are put into a matrix {\bf T} with each row representing an image.
	\item The average of all vectors $\psi$ is calculated and subtracted from each of the vectors in {\bf T} to obtain vectors $\phi_{i}, i = 1,2,..,n$.
	\item The eigenvectors $u_{k}$ and eigenvalues $\lambda_{k}, k = 1,..,M$ of the co-variance matrix {\bf C} are calculated. The covariance matrix itself is found by: 
\begin{equation}
{\bf C} = \frac{1}{M}\sum_{n=1}^{M}\phi_{n}\phi_{n}^{T}
\end{equation}
	\item Since the dimension of C is very high (of the order of the number of pixels in the image), another matrix {\bf L} as analyzed in \cite{Turk91} with the dimensions $M\times M$ is constructed, 
	\begin{equation}
	{\bf L} = {\bf A}^{T}{\bf A}
	\end{equation}
	where 
	\begin{equation}
	{\bf A} = \{\phi_{1},\phi_{2},..,\phi_{M}\}
	\end{equation}
	\item The eigenvectors $v_{l}$ of the matrix {\bf L} are determined such that,
	\begin{equation}
	u_{l} = \sum_{k=1}^{M}v_{lk}\phi_{k} 
	\end{equation}
where $l = 1,..,M$.
\end{enumerate}

To recognize a face, the face image is transformed into its eigenface components. The input image $\tau_{new}$ is compared with the mean image and their difference is multiplied with each eigenvector of the {\bf L} matrix. Each value represents a weight and would be saved on a vector $\Omega$.

\begin{equation}
\omega_{k} = u_{k}^{T}(\tau_{new} - \psi)	\Omega^{T} = [\omega_{1},\omega_{2},..,\omega_{k}] 
\end{equation}

The Euclidiean distance $\varepsilon$ is minimized to determine which face class the new face belongs to. It is computed as follows \cite{eigtut}:
\begin{equation}
\varepsilon_{k} = \parallel\Omega - \Omega_{k}\parallel 
\end{equation}

If $\varepsilon_{k}$ is below an established threshold $\theta_{\varepsilon}$, then the input face is considered to belong to that respective class.


\subsection{Noise Dampening using Support Vector Machines}
Using the output produced by the face recogntion module, the goal is to dampen the noise, by taking advantage of certain features like
\begin{itemize}
	\item Temporal information
	\item Confidence in prediction of recognized face, which in case of Eigenfaces is the Mahalanobis distance of the projected point from its nearest neighbour
	\item Patterns inherent in the noise
\end{itemize}
This is achieved by training a classifier on existing data:
\begin{equation}
\mathcal {D} = \{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\ldots,(x^{(m)},y^{(m)})\}
\end{equation}
where $(x^{(i)}, y^{(i)})$ represents the $i^{th}$ training example in a set of $m$ training examples and $x \in \mathbb{R}^n, y \in \{+1, -1\}$. We generate a hyperplane represented as:
\begin{equation}
h_{w,b}(x) = g(w^Tx + b)
\end{equation}
where
\begin{equation}
g(z) = \left\{
\begin{array}{l l}
+1 & \quad \textit{if $z \geq 0$}\\
-1 & \quad \textit{if $z < 0$}\\
\end{array} \right.
\end{equation}
The \textbf{primal optimization problem} for finding the optimal margin classifier can be stated as:
\begin{equation}
\min_{\gamma, w, b} \frac{1}{2}\parallel w \parallel ^2\\
\end{equation}
subject to the constraint
\begin{equation}
y^{(i)}(w^Tx + b) \geq i = 1, ..., m
\end{equation}
When we construct the Langrangian for this optimization problem, we have:
\begin{equation} \label{eq:lang}
\mathcal {L}(w, b, \alpha) = \frac{1}{2} \parallel w \parallel ^2 - \sum_{i=1}^{m} \alpha_i [y^{(i)}(w^Tx^{(i)} + b) - 1]
\end{equation}
where $\alpha_{i}$ is a Langrange multiplier. To find the \textbf{dual form}, we need to minimize $\mathcal{L}$ which is obtained by differentiating this equation with respect to $w$ and $b$.
Therefore, by taking the derivative with respect to $w$ and setting it to zero:
\begin{equation}
\nabla_{w} \mathcal{L}(w,b,\alpha) = w - \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)} = 0\\
\end{equation}
\begin{equation} \label{eq:w}
\Rightarrow w = \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}\\
\end{equation}
Similarly, the derivative with respect to $b$:
\begin{equation} \label{eq:ay}
\dfrac{\partial}{\partial b} \mathcal{L}(w,b,\alpha) = \sum_{i=1}^{m} \alpha_{i} y^{(i)} = 0
\end{equation}
% \begin{equation} \label{eq:w}
% w = \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}
% \end{equation}
% \begin{equation} \label{eq:ay}
% \sum_{i=1}^{m} \alpha_{i} y^{(i)} = 0
% \end{equation}
Plugging this back into equation \ref{eq:lang}, we get
\begin{equation}
\mathcal {L}(w, b, \alpha) = \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i,j=1}^{m} y^{(i)}y^{(j)}\alpha_i \alpha_j (x^{(i)})^T x^{(j)}
\end{equation}
Putting this together with the constraint $\alpha_i \geq 0$, we obtain the following the dual optimization problem:
\begin{equation} \label{eq:dual}
\max_{\alpha} W(\alpha) = \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i,j=1}^{m} y^{(i)}y^{(j)}\alpha_i \alpha_j \langle x^{(i)}, x^{(j)}\rangle
\end{equation}
subject to the constraints
\begin{equation}
\alpha_{i} \geq 0, i = 1,2,..,m
\end{equation}
and
\begin{equation}
\sum_{i=1}^{m} \alpha_{i}y^{(i)} = 0
\end{equation}
It can be verified that the conditions required for $p^*=d^*$ and the KKT conditions to hold are satisfied in this optimization problem.
By finding the $\alpha$'s given in equation \ref{eq:dual}, which maximizes $W(\alpha)$, the optimal $w$'s can be represented as a function of $\alpha$'s.
Having found $w^{*}$, by considering the primal problem, we obtain the orientation of the hyperplane.
The optimal value of intercept term $b$ can be calculated as:
\begin{equation}
b^{*} = - \frac {max_{i:y^{(i)}=-1}w^{*T}x^{(i)} + min_{i:y^{(i)}=1}w^{*T}x^{(i)}} {2}
\end{equation}

Suppose the model's parameters $w$ and $b$ are fit to the training set, a prediction would require calculate $w^Tx+b$ for a new point $x$. 
This quantity can be written as:
\begin{eqnarray}
w^Tx + b & = & \left( \sum_{i=1}^{m}\alpha_{i} y^{(i)} x^{(i)} \right)^Tx + b \\
         & = & \sum_{i=1}^{m} \alpha_{i}y^{(i)}\langle x^{(i)},x\rangle + b
\end{eqnarray}
Thus, if the value of $\alpha$'s have been calculated, in order to make a prediction, a quantity that only depends on the inner product of the new point and training data needs to be calculated.
Let this quantity be represented as $\langle x, z\rangle$.
Given a feature mapping $\phi$, this inner product can be entire replace by $\langle \phi(x),\phi(z) \rangle$.
A Kernel can now be defined as:
\begin{equation}
K(x,z) = \phi(x)^T\phi(z)
\end{equation}
In our proposed solution, we use radial basis function as the kernel:
\begin{equation}
K(x,z) = exp\left( \frac {\parallel x - z \parallel ^2} {2\sigma^2} \right)
\end{equation}
In the next section we show how the model obtained is used to solve the learning problem.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Implementation
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Implementation} \label{sec:implementation}
The proposed continuous authentication system is implemented on the Linux platform, with majority of the code written in C++ and house-keeping tasks of creating and reorganizing training data written in Python 2.7.
Advanced image processing tasks is handled by OpenCV 2.4\cite{opencv} and the machine learning algorithms is implemented using libSVM\cite{libsvm}.
The system is designed using a client-server model, where the server broadcasts the authentication status of the user.
For the purpose of experimentation and debugging, the information on confidence, logged-in user and the recognized user is displayed on the video feed in a separate window.
Using functions integrated within OpenCV, we are able to implement Viola-Jones face detection algorithm and the Eigenface algorithm.

Few tasks common to both training and testing of hard biometrics traits are the load-store operations, which transfers data to and from our database - an XML file.
These calls are in the form of \verb+cvRead*(..)+ and \verb+cvWrite*(..)+ which either stores or retrieves data in the form of key-value pairs.
The proposed system stores a variety of data including encrypted username-password pairs, eigenvectors, average face image, etc. 

The other task predominantly used is the face detection algorithm.
All the processing done in both the hard and soft biometrics mode is with respect to the face image obtained from face detection.
In case of hard biometrics, the requirement for this task is trivial.
But, in the case of soft biometrics, the shirt region is calculated based on the location of the detected face.
In OpenCV, face detection is implemented as \verb+cvHaarDetectObjects(..)+, which can recognize any object defined by the trained Haar-like features.
The trained Haar-like features for facial recognition tasks are provided by default through OpenCV.

Face recognition in OpenCV is implemented using \verb+cvCalcEigenObjects(..)+ and \verb+cvEigenDecomposite(..)+. 
\verb+cvCalcEigenObjects(..)+ is used to implement PCA on the set of face images in the training dataset.
This PCA subspace is represented as an array of eigenvectors.
With the PCA subspace created, the training images needs to be converted to points in this subspace.
OpenCV provides \verb+cvEigenDecomposite(..)+, which projects the given face image to a point in the PCA subspace.

During \emph{training}, the PCA subspace is created and each face image in the training data is projected into this subspace.
The parameters defining this space and the points are stored in the XML file.\\
During \emph{testing}, the PCA subspace is recreated by loading the trained parameters from the XML file and the test image is projected using \verb+cvEigenDecomposite(..)+.
The nearest neighbour to this projected point is then returned as the recognized user.

As discussed earlier, the noise generated by any face recognition algorithm makes the system incapable of handling a continuous stream of bits, each represented by a authenticated/unauthenticated state.
Hence, our proposed solution dampens this noise using a Support Vector Machine with a Gaussian kernel.
The SVM is implemented using C-SVM provided in libSVM\cite{libsvm}.
The features that the model was trained on were:
\begin{itemize}
	\item Mean confidence over $X_{t-T}$ to $X_{t}$ frames
	\item Mean time since authentication over $X_{t-T}$ to $X_{t}$ frames
	\item $\sum_{i=t-T}^{t}\{Recognized\ user = Logged\-in\ user\}$
\end{itemize}
The reason for using information gathered from the fixed number of frames instead of a time period of fixed length is explained in section \ref{sec:results-svm}.
The predicted authentication states are then logged into a bit-vector $\langle b_0 b_1 b_2 \ldots b_N \rangle$.
The number of bits enabled in this bit-vector dictates the confidence of the system in the hard biometrics mode.
Hence,
\begin{eqnarray}
\theta_{H} & = & \frac {\textit{No. of bits enabled}} {\textit{Length of bit-vector}}\\
           & = & \frac{\sum_{i=0}^{N} 1\{b_i=1\}}{N}
\end{eqnarray}

The soft biometrics represented by $\theta_{S}$ is calculated based on the similarity between the current template and the enrolled template.
The template here refers to a vector calculated over a fixed number of frames, whose histogram values have been averaged.
The similarity/confidence is computed using normalized root mean square difference.
 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Results
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Results} \label{sec:results}
In this section, we study the performance and discuss the design choices made for each module described previously in the section \ref{sec:arch}.
For the rest of the section, the face detection and face recognition algorithm implies Viola-Jones' method and Eigenfaces respectively.

\subsection{Hard Biometrics}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/avg.jpeg}
	\caption{Average image generated by Eigenfaces}
	\label{fig:avg}
\end{figure}
During the Eigenface training phase, for the dataset to be "centered" during PCA, an average image as shown in figure \ref{fig:avg} is computed by calculating the mean of the pixels of all the images in the training data set. 
The faces are then represented as a composition of the average face and a weighted average of the eigenface features as seen in \ref{fig:eigen}.
For example, a person might be characterized as the average plus 20\% from eigenface 1, 12\% from eigenface 2 and so on.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.15]{img/eigen.png}
	\caption{The first few dominant Eigenfaces}
	\label{fig:eigen}
\end{figure}
Figure \ref{fig:avg} and \ref{fig:eigen} were generated using 250 images, with each of the 10 users contributing 25 images.
It can be seen that the average image shows a smooth face structure, the first few eigenfaces shows some of the domninant traits and later on it captures mostly noise and hence the contributions from these are negligible.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/fd_fr_fdfr.png}
	\caption{Comparison of the facial features processing tasks}
	\label{fig:fdfr}
\end{figure}
Figure \ref{fig:fdfr} was obtained by running the continuous authentication system in a hard biometrics only mode under normal conditions. The average time taken to process each frame for the task of face detection on an entire frame and face recognition for a face image are shown in \ref{tab:fdr}

\begin{table}[htp]
	\centering
	\caption{Facial features processing time}
	\begin{tabular}{||l|c||} \hline \hline
			    &  Average time taken \\ \hline
	Face detection      &  0.0224             \\ \hline
	Face recognition    &  0.0454             \\ \hline \hline
	\end{tabular}
	\label{tab:fdr}
\end{table}
In practise, all operations(including face recognition using Eigenfaces) on the image are performed by retrieving the face resized to a fixed height and width.

As mentioned earlier, the accuracy of Eigenfaces is affected by contrasting changes in illumination and posture.
This can be seen in figure \ref{fig:fracc} where the y-axis represents the function $1\{user_{recognized}=user_{authorized}\}$ and UID refers to the User-ID of the respective users in the database.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/face_rec_accuracy.png}
	\caption{Accuracy of face recognition achieved using Eigenfaces}
	\label{fig:fracc}
\end{figure}

\subsection{Soft biometrics}
Figure \ref{fig:fsoft} empirically justifies our reason to transition into soft biometrics mode when necessary.
We observed that after retrieving the face image of the person in front of the system, the soft biometrics consumes half as much time as face recognition.
Note that the time represented for both the observations in figure \ref{fig:fsoft} includes the time taken to pre-process the frame and retrieve the face image.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/face_vs_soft.png}
	\caption{Comparison of Face recognition and Soft biometrics}
	\label{fig:fsoft}
\end{figure}
The average time consumed per frame as observed can be seen in table \ref{tab:frsb}.
As shown, an accuracy of 80\% given that the user was unaware of the colour of the shirt worn by the user.
Such a condition was assumed since our work provides a solution to extend the soft biometrics to include more modes.
\begin{table}[htp]
	\centering
	\caption{Face recognition vs. Soft biometrics}
	\begin{tabular}{||l|c|c||} \hline \hline
	-                  &  Average time  &  Accuracy \\ \hline
	Face recognition   &  0.12          &  60-80\% \\ \hline
	Soft Biometrics    &  0.045         &  80\% using $\theta_{S}=0.75$ \\ \hline \hline
	\end{tabular}
	\label{tab:frsb}
\end{table}

In figures \ref{fig:soft1} and \ref{fig:softa}, the confidence over time was observed in two cases - when the authenticated user was in front of the system and when he is tailgated by an imposter.
By varying $T$, the template at time $t$ compared the enrolled template can be smoothened out.
This can be seen in both the figures where the peaks are dampened out as a result of increasing $T$.

\begin{figure}
	\centering
	\subfloat[Authenticated user is in front of the system]{%
		\label{fig:soft1}%
		\includegraphics[scale=0.40]{img/soft_conf.png}}
	\quad
	\subfloat[Authenticated user is tailgated]{%
		\label{fig:softa}%
		\includegraphics[scale=0.40]{img/soft_conf_away.png}}
	\caption{Variation of Soft Biometric confidence}
\end{figure}

\subsection{Noise dampening using SVM} \label{sec:results-svm}
Figure \ref{fig:svm1} captures the output of indicator function as described earlier.
As seen from this figure, the immense noise in the data cannot solely be the basis for predicting if the user in front of the system is the authenticated user.
This noise exists as a result of false positives and false negatives from the prediction. 

\begin{figure}
	\centering
	\subfloat[Output received from the Face recognition module]{%
		\label{fig:svm1}%
		\includegraphics[scale=0.40]{img/no_svm.png}}
	\quad
	\subfloat[Confidence estimated by SVM using Face recognition data]{%
		\label{fig:svm2}%
		\includegraphics[scale=0.40]{img/svm.png}}
	\caption{Face recognition output comparison with and without SVM}
\end{figure}

Our solution overcomes this by extracting temporal information and the confidence estimated by the face recognition algorithm.
Various situations were modeled and the data extracted was used in training to develop a classifier.
The output produced by this classifier is then represented as confidence, which can be seen in figure \ref{fig:svm2}.

In this work, the temporal information is extracted based on a fixed number of previous frames rather than a time period of fixed length.
This is not only because the number of frames processed varies among these time periods as seen in figure \ref{fig:ftime}, but is also dependent on the system's resources and other conditions.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.40]{img/t_fno.png}
	\caption{Frames processed with respect to time}
	\label{fig:ftime}
\end{figure}

\subsection{Continuous Authentication mode}
\begin{figure}
	\centering
	\subfloat[]{%
		\label{fig:trans1}%
		\includegraphics[scale=0.40]{img/trans_1.png}}
	\quad
	\subfloat[]{%
		\label{fig:trans2}%
		\includegraphics[scale=0.40]{img/trans_2.png}}
	\caption{Confidence as estimated by the entire system}
\end{figure}
In this subsection, we briefly look into how the control flow jumps between the hard and soft biometrics modules.
Figure \ref{fig:trans1} was plotted by alternating the authenticated user and an imposter in front of the system.
This resulted in a transition from hard to soft biometrics when the system was confident, and the other way round when the system needed to re-inforce its belief.
Figure \ref{fig:trans2} models real-world condition where in the authenticated user after login moves away for a break and an imposter takes his place.
Thus, the soft biometrics mode confidence drops which activates the hard biometrics module.
The predictions made here prove that the authenticated user has been tailgated.

Another notable result as published in \cite{fdetGPU,cudafrec} is the performance of the Viola-Jones face detection and Eigenfaces face recognition algorithms when ported on to a GPU. As per \cite{fdetGPU}, a CPU-GPU cooperative implementation of the Viola-Jones face detection algorithm on GTX280 graphics card achieved speed-ups of over 20$\times$ as compared to its implementation on the Intel Core 2 Duo CPU alone. As for Eigenfaces, it was shown by \cite{cudafrec} that a highly parallelised implementation of Eigenfaces achieved highest speedups on GeForce GTX 480 for database of 15,000 images. A speedup of 207$\times$ was achieved for extraction of feature vectors in training process while the same for the recognition pipeline was shown to be 330$\times$. Overall testing process yielded a speedup of 165$\times$ (testing over 40 images). The OpenCV implementations of both these algorithms can be modified to run on a GPGPU architecture to achieve a boost in their performance, thereby improving the performance of the proposed Multimodal Continuous Authentication System, if the parallelized implementations are considered. 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% Conclusion and Future enhancements
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Conclusion and Future enhancements} \label{sec:conclusion}
In order to continuously authenticate the user, we presented an approach of alternating between two modes - Hard biometrics and Soft biometrics.
The control flow transtitions between these modes depending on the need to re-inforce the system's belief.
Efforts made to increase the accuracy of face recognition algorithms fail to take into account the need to continuously recognize the user throughout a session.
The result of this was seen in section \ref{sec:results}, where the predictions made become noisy due to variations in postures, since the user is unaware of the recognition process in the background.
By learning from temporal data using Support Vector Machines, we attenuate this noise and make confident predictions over a stretch of time.
The framework used also provides flexibility in replacing as well as extending the current setup to include other modules by transitioning based on only confidence of each mode.

Our future work in this field involves enhancements such as:
\begin{itemize}
\item Taking advantage of the enormous speed-ups when face detection and recognition are processed on the GPU.
\item The training of the user's face model can be improved by introducing Online Training where each time the user is authenticated with a high level of confidence, a few face images are captured and the face model is retrained.
\item The whole system can be implemented on a distributed architecture, with the face database in a central repository and the users logging in via different nodes associated with this central repository. The Client - Server architecture may be implemented for this.
\item The Soft Biometric Traits may be expanded to include more features such as the complexion of the user, eye colour or other facial features like facial hair. 
\item Support for multiple users sharing a certain account; this may require a biometric handoff\cite{Klos00} to occur between users.
\item Improve accuracy of face recognition under varied lighting conditions by implementing recognition using a different algorithm or approach since face recognition has been implemented as a separate module.
\item Make provision for recognizing any kind of tampering occuring to the video feed, so as to prevent authenticating imposters. This can be done by restricting access to the webcam feed via parameters that define access to it.
\end{itemize}
% do the biliography:
\bibliographystyle{IEEEbib}
\input{tutref}

\end{document}
